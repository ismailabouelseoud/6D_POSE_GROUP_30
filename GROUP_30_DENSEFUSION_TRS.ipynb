{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_gBct7sjAnK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install open3d\n",
        "!pip install ruamel.yaml\n",
        "!pip install trimesh\n",
        "!pip install ultralytics\n",
        "!pip install pcl\n",
        "!pip install pyyaml\n",
        "!pip install plotly\n",
        "\n",
        "# replace with yolo detection position if you want to skip finetuning\n",
        "#!unzip -q \"/content/drive/MyDrive/2024-25_S2/01TXFSM - MLADL/04_3DPE_PROJECT/04_COLAB_NOTEBOOK/01_DETECTION/02_YOLO/YOLOv11_finetuning.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7axrr8c-YAk"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 1: LIBRARIES IMPORTING AND DATASET PREPROCESSING FOR YOLO FINE-TUNING\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import open3d as o3d\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from matplotlib.patches import Rectangle\n",
        "from ruamel.yaml import YAML\n",
        "from ruamel.yaml.comments import CommentedMap\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from scipy.spatial import cKDTree\n",
        "import trimesh\n",
        "import subprocess\n",
        "import datetime\n",
        "import gc\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION - MODIFY THESE PATHS FOR YOUR SETUP\n",
        "# ==============================================================================\n",
        "# Dataset paths\n",
        "#\n",
        "#CHANGE THIS DIRECTORY WRT YOUR GDRIVE\n",
        "LINEMOD_ZIP_PATH = \"/content/drive/MyDrive/2024-25_S2/01TXFSM - MLADL/04_3DPE_PROJECT/03_Docs/00_DATASET/Linemod_preprocessed.zip\"\n",
        "LINEMOD_ROOT = \"/content/datasets/linemod\"\n",
        "YOLO_DATASET_ROOT = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2\"\n",
        "\n",
        "# Model paths\n",
        "PLY_MODELS_DIR = \"/content/Linemod_preprocessed/models\"\n",
        "FINAL_MODEL_DIR = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2/pose_models\"\n",
        "\n",
        "# Dataset configuration\n",
        "OBJECT_IDS = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n",
        "OBJECT_NAMES = ['ape', 'benchvise', 'camera', 'can', 'cat', 'driller', 'duck', 'eggbox', 'glue', 'holepuncher', 'iron', 'lamp', 'phone']\n",
        "OBJECTS_TO_SKIP = [\"03\", \"07\"]  # Objects removed in DenseFusion preprocessing\n",
        "\n",
        "# Dataset split ratios (following DenseFusion paper)\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Depth processing configuration\n",
        "INCLUDE_DEPTH = True\n",
        "DEPTH_SUBFOLDER = \"depth\"\n",
        "DEPTH_SCALE_FACTORS = {}  # Will store depth scale factors for each object\n",
        "\n",
        "# ==============================================================================\n",
        "# INITIAL SETUP\n",
        "# ==============================================================================\n",
        "def extract_and_setup_dataset():\n",
        "    \"\"\"Extract dataset and create directory structure\"\"\"\n",
        "    print(\"Setting up Linemod dataset...\")\n",
        "\n",
        "    # Extract dataset\n",
        "    os.system(f'unzip -q \"{LINEMOD_ZIP_PATH}\"')\n",
        "\n",
        "    # Create directory structure\n",
        "    os.makedirs(f\"{LINEMOD_ROOT}/data\", exist_ok=True)\n",
        "    os.system(f'mv \"/content/Linemod_preprocessed/data\" \"{LINEMOD_ROOT}/\"')\n",
        "\n",
        "    print(\"✓ Dataset extraction completed\")\n",
        "\n",
        "# ==============================================================================\n",
        "# DATASET CONVERSION UTILITIES\n",
        "# ==============================================================================\n",
        "\n",
        "def create_yolo_directories():\n",
        "    \"\"\"Create YOLO dataset directory structure\"\"\"\n",
        "    directories = [\n",
        "        'images/train', 'images/val', 'images/test',\n",
        "        'labels/train', 'labels/val', 'labels/test'\n",
        "    ]\n",
        "\n",
        "    if INCLUDE_DEPTH:\n",
        "        directories.extend([\n",
        "            'depth/train', 'depth/val', 'depth/test',\n",
        "            'metadata'\n",
        "        ])\n",
        "\n",
        "    for directory in directories:\n",
        "        os.makedirs(os.path.join(YOLO_DATASET_ROOT, directory), exist_ok=True)\n",
        "\n",
        "    print(\"✓ YOLO directory structure created\")\n",
        "\n",
        "def get_all_samples(dataset_root):\n",
        "    \"\"\"Retrieve all available sample indices from object folders\"\"\"\n",
        "    samples = []\n",
        "    data_path = os.path.join(dataset_root, 'data')\n",
        "\n",
        "    for folder_id in OBJECT_IDS:\n",
        "        rgb_folder = os.path.join(data_path, folder_id, \"rgb\")\n",
        "        if os.path.exists(rgb_folder):\n",
        "            sample_ids = sorted([\n",
        "                int(os.path.splitext(f)[0])\n",
        "                for f in os.listdir(rgb_folder)\n",
        "                if f.endswith('.png')\n",
        "            ])\n",
        "            samples.extend([(folder_id, sid) for sid in sample_ids])\n",
        "\n",
        "    return samples\n",
        "\n",
        "def load_yaml_file(file_path):\n",
        "    \"\"\"Load YAML file with error handling\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            return yaml.load(f, Loader=yaml.FullLoader)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def convert_bbox_to_yolo(bbox_linemod, image_width, image_height):\n",
        "    \"\"\"\n",
        "    Convert Linemod bbox [x_min, y_min, width, height] to YOLO format\n",
        "    YOLO format: [center_x, center_y, width, height] normalized to [0,1]\n",
        "    \"\"\"\n",
        "    if not isinstance(bbox_linemod, (list, np.ndarray)) or len(bbox_linemod) != 4:\n",
        "        return None\n",
        "\n",
        "    x_min, y_min, width_px, height_px = bbox_linemod\n",
        "\n",
        "    # Convert to normalized center coordinates\n",
        "    center_x = (x_min + width_px / 2.0) / image_width\n",
        "    center_y = (y_min + height_px / 2.0) / image_height\n",
        "    width_norm = width_px / image_width\n",
        "    height_norm = height_px / image_height\n",
        "\n",
        "    # Clamp to valid range\n",
        "    center_x = max(0.0, min(1.0, center_x))\n",
        "    center_y = max(0.0, min(1.0, center_y))\n",
        "    width_norm = max(0.0, min(1.0, width_norm))\n",
        "    height_norm = max(0.0, min(1.0, height_norm))\n",
        "\n",
        "    if width_norm <= 0 or height_norm <= 0:\n",
        "        return None\n",
        "\n",
        "    return [center_x, center_y, width_norm, height_norm]\n",
        "\n",
        "def load_depth_scale_factor(dataset_root, folder_id):\n",
        "    \"\"\"Load depth scale factor for proper depth-to-meters conversion\"\"\"\n",
        "    camera_path = os.path.join(dataset_root, 'data', folder_id, 'camera.yml')\n",
        "\n",
        "    if os.path.exists(camera_path):\n",
        "        try:\n",
        "            camera_data = load_yaml_file(camera_path)\n",
        "            if camera_data:\n",
        "                return camera_data.get('depth_scale', 1000.0)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return 1000.0  # Default: mm to meters\n",
        "\n",
        "def process_depth_image(depth_src_path, depth_target_path):\n",
        "    \"\"\"Process and link depth image\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(depth_target_path):\n",
        "            os.remove(depth_target_path)\n",
        "        os.symlink(depth_src_path, depth_target_path)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN CONVERSION PROCESS\n",
        "# ==============================================================================\n",
        "\n",
        "def convert_linemod_to_yolo():\n",
        "    \"\"\"Convert Linemod dataset to YOLO format\"\"\"\n",
        "    print(\"Starting Linemod to YOLO conversion...\")\n",
        "\n",
        "    # Debug: Check if data directory exists\n",
        "    data_dir = os.path.join(LINEMOD_ROOT, 'data')\n",
        "    print(f\"Checking data directory: {data_dir}\")\n",
        "    print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n",
        "\n",
        "    if os.path.exists(data_dir):\n",
        "        subdirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "        print(f\"Found subdirectories: {subdirs}\")\n",
        "\n",
        "        # Check RGB folders\n",
        "        for subdir in subdirs[:3]:  # Check first few\n",
        "            rgb_dir = os.path.join(data_dir, subdir, 'rgb')\n",
        "            print(f\"  {subdir}/rgb exists: {os.path.exists(rgb_dir)}\")\n",
        "            if os.path.exists(rgb_dir):\n",
        "                files = os.listdir(rgb_dir)[:5]  # Show first few files\n",
        "                print(f\"    Sample files: {files}\")\n",
        "\n",
        "    # Get all samples and split dataset\n",
        "    all_samples = get_all_samples(LINEMOD_ROOT)\n",
        "    print(f\"Found {len(all_samples)} total samples\")\n",
        "\n",
        "    # Split dataset (following DenseFusion methodology)\n",
        "    train_samples, temp_samples = train_test_split(\n",
        "        all_samples, train_size=TRAIN_RATIO, random_state=RANDOM_SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    if TEST_RATIO > 0:\n",
        "        val_size_relative = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "        val_samples, test_samples = train_test_split(\n",
        "            temp_samples, train_size=val_size_relative, random_state=RANDOM_SEED\n",
        "        )\n",
        "    else:\n",
        "        val_samples = temp_samples\n",
        "        test_samples = []\n",
        "\n",
        "    print(f\"Dataset split: {len(train_samples)} train, {len(val_samples)} val, {len(test_samples)} test\")\n",
        "\n",
        "    # Create sample-to-split mapping\n",
        "    sample_to_split = {}\n",
        "    for s in train_samples:\n",
        "        sample_to_split[s] = 'train'\n",
        "    for s in val_samples:\n",
        "        sample_to_split[s] = 'val'\n",
        "    for s in test_samples:\n",
        "        sample_to_split[s] = 'test'\n",
        "\n",
        "    # Load depth scale factors\n",
        "    if INCLUDE_DEPTH:\n",
        "        for folder_id in OBJECT_IDS:\n",
        "            DEPTH_SCALE_FACTORS[folder_id] = load_depth_scale_factor(LINEMOD_ROOT, folder_id)\n",
        "\n",
        "    # Cache YAML data\n",
        "    gt_cache = {}\n",
        "    info_cache = {}\n",
        "\n",
        "    def get_cached_yaml(folder_id):\n",
        "        if folder_id not in gt_cache:\n",
        "            gt_cache[folder_id] = load_yaml_file(\n",
        "                os.path.join(LINEMOD_ROOT, 'data', folder_id, 'gt.yml')\n",
        "            )\n",
        "        if folder_id not in info_cache:\n",
        "\n",
        "            info_cache[folder_id] = load_yaml_file(\n",
        "                os.path.join(LINEMOD_ROOT, 'data', folder_id, 'info.yml')\n",
        "            )\n",
        "\n",
        "        return gt_cache[folder_id], info_cache[folder_id]\n",
        "\n",
        "    # Process samples\n",
        "    processed_count = 0\n",
        "    samples_with_annotations = 0\n",
        "    total_annotations = 0\n",
        "    depth_processed = 0\n",
        "\n",
        "    # Map object folder IDs to class IDs (1-based indexing: 1 to 13)\n",
        "    object_id_to_class_id = {obj_id: i+1 for i, obj_id in enumerate(OBJECT_IDS)}\n",
        "\n",
        "    print(f\"Processing {len(all_samples)} samples...\")\n",
        "    print(f\"Object ID mapping (1-based): {object_id_to_class_id}\")\n",
        "\n",
        "    for sample_idx, (folder_id, sample_id) in enumerate(all_samples):\n",
        "        split_name = sample_to_split.get((folder_id, sample_id))\n",
        "        if not split_name:\n",
        "            print(f\"Warning: Sample ({folder_id}, {sample_id}) not found in split mapping\")\n",
        "            continue\n",
        "\n",
        "        # Load YAML data\n",
        "        gt_data, info_data = get_cached_yaml(folder_id)\n",
        "        if not gt_data or not info_data:\n",
        "            continue\n",
        "\n",
        "        # Construct file paths\n",
        "        img_filename = f\"{sample_id:04d}.png\"\n",
        "        img_src_path = os.path.join(LINEMOD_ROOT, 'data', folder_id, 'rgb', img_filename)\n",
        "\n",
        "        if not os.path.exists(img_src_path):\n",
        "            continue\n",
        "\n",
        "        # Linemod standard image dimensions (all images are the same size)\n",
        "        image_width = 640\n",
        "        image_height = 480\n",
        "\n",
        "        # Process annotations\n",
        "        image_annotations = gt_data.get(sample_id, [])\n",
        "\n",
        "\n",
        "        yolo_lines = []\n",
        "\n",
        "        if not image_annotations:\n",
        "            print(f\"Debug: No annotations found for sample {sample_id} in folder {folder_id}\")\n",
        "\n",
        "        for annotation in image_annotations:\n",
        "            obj_id = annotation.get('obj_id')\n",
        "            folder_match = f\"{obj_id:02d}\"\n",
        "            if folder_match == folder_id:\n",
        "\n",
        "                bbox = annotation.get('obj_bb')\n",
        "                cam_R = annotation.get('cam_R_m2c')\n",
        "                cam_t = annotation.get('cam_t_m2c')\n",
        "                R11, R12, R13, R21, R22, R23, R31, R32, R33  = cam_R\n",
        "                tx, ty, tz = cam_t\n",
        "                if obj_id is None or bbox is None:\n",
        "                    print(f\"Warning: Invalid annotation for sample {sample_id}: obj_id={obj_id}, bbox={bbox}\")\n",
        "                    continue\n",
        "\n",
        "                # Map object ID to class ID\n",
        "                folder_match = f\"{obj_id:02d}\"\n",
        "                if folder_match not in OBJECT_IDS:\n",
        "                    print(f\"Warning: Object ID {obj_id} ({folder_match}) not in OBJECT_IDS for sample {sample_id}\")\n",
        "                    continue\n",
        "\n",
        "                class_id = object_id_to_class_id[folder_match]\n",
        "                yolo_bbox = convert_bbox_to_yolo(bbox, image_width, image_height)\n",
        "\n",
        "                if yolo_bbox:\n",
        "                    line = f\"{class_id} {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\"\n",
        "                    yolo_lines.append(line)\n",
        "                    line = f\"{class_id} {R11:.6f} {R12:.6f} {R13:.6f} {R21:.6f} {R22:.6f} {R23:.6f} {R31:.6f} {R32:.6f} {R33:.6f}\"\n",
        "                    yolo_lines.append(line)\n",
        "                    line = f\"{class_id} {tx:.6f} {ty:.6f} {tz:.6f}\"\n",
        "                    yolo_lines.append(line)\n",
        "\n",
        "                    total_annotations += 1\n",
        "\n",
        "        if yolo_lines:\n",
        "            samples_with_annotations += 1\n",
        "\n",
        "        # Create target paths\n",
        "        new_img_name = f\"{sample_idx:05d}.png\"\n",
        "        new_label_name = f\"{sample_idx:05d}.txt\"\n",
        "\n",
        "        img_target_path = os.path.join(YOLO_DATASET_ROOT, 'images', split_name, new_img_name)\n",
        "        label_target_path = os.path.join(YOLO_DATASET_ROOT, 'labels', split_name, new_label_name)\n",
        "\n",
        "        # Write label file\n",
        "        try:\n",
        "            with open(label_target_path, 'w') as f:\n",
        "                for line in yolo_lines:\n",
        "                    f.write(line + '\\n')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not write label file {label_target_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Link image file\n",
        "        try:\n",
        "            if os.path.exists(img_target_path):\n",
        "                os.remove(img_target_path)\n",
        "            os.symlink(img_src_path, img_target_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create symlink for {img_src_path}: {e}\")\n",
        "            # Try copying instead\n",
        "            try:\n",
        "                import shutil\n",
        "                shutil.copy2(img_src_path, img_target_path)\n",
        "            except Exception as e2:\n",
        "                print(f\"Warning: Could not copy image {img_src_path}: {e2}\")\n",
        "                continue\n",
        "\n",
        "        # Process depth if enabled\n",
        "        if INCLUDE_DEPTH:\n",
        "            depth_src_path = os.path.join(LINEMOD_ROOT, 'data', folder_id, DEPTH_SUBFOLDER, img_filename)\n",
        "            if os.path.exists(depth_src_path):\n",
        "                depth_target_path = os.path.join(YOLO_DATASET_ROOT, 'depth', split_name, new_img_name)\n",
        "                if process_depth_image(depth_src_path, depth_target_path):\n",
        "                    depth_processed += 1\n",
        "\n",
        "        processed_count += 1\n",
        "\n",
        "        # Progress reporting\n",
        "        if processed_count % 2000 == 0:\n",
        "            print(f\"Processed {processed_count}/{len(all_samples)} samples...\")\n",
        "\n",
        "    print(f\"Conversion completed:\")\n",
        "    print(f\"  Processed: {processed_count} samples\")\n",
        "    print(f\"  Annotations: {total_annotations}\")\n",
        "    print(f\"  Samples with annotations: {samples_with_annotations}\")\n",
        "    if INCLUDE_DEPTH:\n",
        "        print(f\"  Depth images: {depth_processed}\")\n",
        "\n",
        "def create_data_yaml():\n",
        "    \"\"\"Create YOLO dataset configuration file\"\"\"\n",
        "    config_content = f\"\"\"# Linemod Dataset Configuration for DenseFusion\n",
        "# Following original paper specifications\n",
        "\n",
        "# Image directories\n",
        "train: {os.path.join(YOLO_DATASET_ROOT, 'images', 'train')}\n",
        "val: {os.path.join(YOLO_DATASET_ROOT, 'images', 'val')}\n",
        "test: {os.path.join(YOLO_DATASET_ROOT, 'images', 'test')}\n",
        "\"\"\"\n",
        "\n",
        "    if INCLUDE_DEPTH:\n",
        "        config_content += f\"\"\"\n",
        "# Depth directories\n",
        "depth_train: {os.path.join(YOLO_DATASET_ROOT, 'depth', 'train')}\n",
        "depth_val: {os.path.join(YOLO_DATASET_ROOT, 'depth', 'val')}\n",
        "depth_test: {os.path.join(YOLO_DATASET_ROOT, 'depth', 'test')}\n",
        "\n",
        "# Depth metadata\n",
        "depth_scales: {os.path.join(YOLO_DATASET_ROOT, 'metadata', 'depth_scales.json')}\n",
        "\"\"\"\n",
        "\n",
        "    config_content += f\"\"\"\n",
        "# Object classes (13 objects after removing 03 and 07)\n",
        "nc: {len(OBJECT_IDS)}\n",
        "names: {OBJECT_NAMES}\n",
        "\n",
        "# Class mapping: 1-13 (not 0-12)\n",
        "# Class 1: ape, Class 2: benchvise, Class 3: camera, etc.\n",
        "\"\"\"\n",
        "\n",
        "    config_path = os.path.join(YOLO_DATASET_ROOT, 'data.yaml')\n",
        "    with open(config_path, 'w') as f:\n",
        "        f.write(config_content)\n",
        "\n",
        "    # Save depth scale factors if enabled\n",
        "    if INCLUDE_DEPTH:\n",
        "        depth_metadata_path = os.path.join(YOLO_DATASET_ROOT, 'metadata', 'depth_scales.json')\n",
        "        with open(depth_metadata_path, 'w') as f:\n",
        "            json.dump(DEPTH_SCALE_FACTORS, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Created data.yaml configuration\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3D MODEL PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "def process_ply_models():\n",
        "    \"\"\"Process and renumber PLY model files\"\"\"\n",
        "    print(\"Processing 3D PLY models...\")\n",
        "\n",
        "    # Find and filter PLY files\n",
        "    all_ply_files = glob.glob(os.path.join(PLY_MODELS_DIR, \"obj_*.ply\"))\n",
        "\n",
        "    files_to_process = []\n",
        "    for ply_path in all_ply_files:\n",
        "        base_name = os.path.basename(ply_path)\n",
        "        match = re.match(r\"obj_(\\d+)\\.ply\", base_name)\n",
        "\n",
        "        if match:\n",
        "            file_number = match.group(1)\n",
        "            if file_number not in OBJECTS_TO_SKIP:\n",
        "                files_to_process.append((int(file_number), ply_path))\n",
        "\n",
        "    # Sort and renumber\n",
        "    files_to_process.sort(key=lambda x: x[0])\n",
        "\n",
        "    for new_index, (original_number, original_path) in enumerate(files_to_process):\n",
        "        new_number = new_index + 1\n",
        "        new_filename = f\"obj_{new_number:02d}.ply\"\n",
        "        new_path = os.path.join(PLY_MODELS_DIR, new_filename)\n",
        "\n",
        "        if original_path != new_path:\n",
        "            os.rename(original_path, new_path)\n",
        "\n",
        "    print(f\"✓ Processed {len(files_to_process)} PLY models\")\n",
        "\n",
        "def process_model_info_yaml():\n",
        "    \"\"\"Process models_info.yml to match renumbered objects\"\"\"\n",
        "    os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    # Move and process models_info.yml\n",
        "    source_yml = os.path.join(PLY_MODELS_DIR, \"models_info.yml\")\n",
        "    target_yml = os.path.join(FINAL_MODEL_DIR, \"models_info.yml\")\n",
        "\n",
        "    shutil.move(source_yml, target_yml)\n",
        "\n",
        "    # Load and modify YAML\n",
        "    yaml_processor = YAML()\n",
        "    yaml_processor.preserve_quotes = True\n",
        "    yaml_processor.width = sys.maxsize\n",
        "\n",
        "    with open(target_yml, \"r\") as f:\n",
        "        data = yaml_processor.load(f)\n",
        "\n",
        "    # Remove objects 3 and 7\n",
        "    for key in [3, 7]:\n",
        "        data.pop(key, None)\n",
        "\n",
        "    # Set flow style for dictionaries\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            cm = CommentedMap(value)\n",
        "            cm.fa.set_flow_style()\n",
        "            data[key] = cm\n",
        "\n",
        "    # Renumber keys consecutively starting from 1\n",
        "    new_data = CommentedMap()\n",
        "    for i, (old_key, value) in enumerate(sorted(data.items()), 1):\n",
        "        if isinstance(value, dict):\n",
        "            cm = CommentedMap(value)\n",
        "            cm.fa.set_flow_style()\n",
        "            new_data[i] = cm\n",
        "        else:\n",
        "            new_data[i] = value\n",
        "\n",
        "    # Save updated YAML\n",
        "    with open(target_yml, \"w\") as f:\n",
        "        yaml_processor.dump(new_data, f)\n",
        "\n",
        "    print(\"✓ Processed models_info.yml\")\n",
        "\n",
        "def setup_model_directories():\n",
        "    \"\"\"Create necessary model directories for DenseFusion (simplified)\"\"\"\n",
        "    global PLY_MODELS_DIR  # Declare global at the start\n",
        "\n",
        "    # Only create directories we actually need\n",
        "    directories = [\n",
        "        'pose_models/models',      # 3D PLY models\n",
        "        'trained_models',          # Saved DenseFusion models\n",
        "        'checkpoints'              # Training checkpoints\n",
        "    ]\n",
        "\n",
        "    for directory in directories:\n",
        "        dir_path = os.path.join(YOLO_DATASET_ROOT, directory)\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        print(f\"  Created: {directory}\")\n",
        "\n",
        "    # Debug: Check if source PLY directory exists and has files\n",
        "    print(f\"Checking source PLY directory: {PLY_MODELS_DIR}\")\n",
        "    print(f\"Source PLY dir exists: {os.path.exists(PLY_MODELS_DIR)}\")\n",
        "\n",
        "    if os.path.exists(PLY_MODELS_DIR):\n",
        "        ply_files = [f for f in os.listdir(PLY_MODELS_DIR) if f.endswith('.ply')]\n",
        "        print(f\"Found PLY files: {ply_files}\")\n",
        "\n",
        "        # Copy PLY models to final location\n",
        "        ply_target = os.path.join(FINAL_MODEL_DIR, 'models')\n",
        "        print(f\"Target PLY directory: {ply_target}\")\n",
        "\n",
        "        # Create target directory if it doesn't exist\n",
        "        os.makedirs(ply_target, exist_ok=True)\n",
        "\n",
        "        # Copy each PLY file individually (more reliable than copytree)\n",
        "        copied_count = 0\n",
        "        for ply_file in ply_files:\n",
        "            src_path = os.path.join(PLY_MODELS_DIR, ply_file)\n",
        "            dst_path = os.path.join(ply_target, ply_file)\n",
        "            try:\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "                copied_count += 1\n",
        "                print(f\"    Copied: {ply_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Failed to copy {ply_file}: {e}\")\n",
        "\n",
        "        print(f\"  Copied {copied_count}/{len(ply_files)} PLY models to: {ply_target}\")\n",
        "\n",
        "        # Verify the copy worked\n",
        "        if os.path.exists(ply_target):\n",
        "            copied_files = [f for f in os.listdir(ply_target) if f.endswith('.ply')]\n",
        "            print(f\"  Verification: {len(copied_files)} PLY files in target directory\")\n",
        "\n",
        "    else:\n",
        "        print(f\"  WARNING: Source PLY directory not found: {PLY_MODELS_DIR}\")\n",
        "\n",
        "        # Try to find PLY files in alternate locations\n",
        "        alternate_locations = [\n",
        "            \"/content/Linemod_preprocessed/models\",\n",
        "            \"/content/datasets/linemod/models\",\n",
        "            \"/content/datasets/linemod/Linemod_preprocessed/models\"\n",
        "        ]\n",
        "\n",
        "        for alt_path in alternate_locations:\n",
        "            print(f\"  Checking alternate location: {alt_path}\")\n",
        "            if os.path.exists(alt_path):\n",
        "                ply_files = [f for f in os.listdir(alt_path) if f.endswith('.ply')]\n",
        "                if ply_files:\n",
        "                    print(f\"    Found {len(ply_files)} PLY files at: {alt_path}\")\n",
        "                    print(f\"    Files: {ply_files}\")\n",
        "\n",
        "                    # Update the global variable\n",
        "                    PLY_MODELS_DIR = alt_path\n",
        "\n",
        "                    # Copy from this location\n",
        "                    ply_target = os.path.join(FINAL_MODEL_DIR, 'models')\n",
        "                    os.makedirs(ply_target, exist_ok=True)\n",
        "\n",
        "                    copied_count = 0\n",
        "                    for ply_file in ply_files:\n",
        "                        src_path = os.path.join(alt_path, ply_file)\n",
        "                        dst_path = os.path.join(ply_target, ply_file)\n",
        "                        try:\n",
        "                            shutil.copy2(src_path, dst_path)\n",
        "                            copied_count += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"      Failed to copy {ply_file}: {e}\")\n",
        "\n",
        "                    print(f\"    Copied {copied_count} PLY files from alternate location\")\n",
        "                    break\n",
        "\n",
        "    print(\"✓ Essential model directories created\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main preprocessing pipeline\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"LINEMOD DATASET PREPROCESSING FOR DENSEFUSION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Extract and setup dataset\n",
        "    extract_and_setup_dataset()\n",
        "\n",
        "    # Step 2: Create YOLO directory structure\n",
        "    create_yolo_directories()\n",
        "\n",
        "    # Step 3: Convert dataset to YOLO format\n",
        "    convert_linemod_to_yolo()\n",
        "\n",
        "    # Step 4: Create configuration files\n",
        "    create_data_yaml()\n",
        "\n",
        "    # Step 5: Process 3D models\n",
        "    process_ply_models()\n",
        "    process_model_info_yaml()\n",
        "    setup_model_directories()\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Dataset ready for DenseFusion training:\")\n",
        "    print(f\"  YOLO dataset: {YOLO_DATASET_ROOT}\")\n",
        "    print(f\"  3D models: {FINAL_MODEL_DIR}\")\n",
        "    print(f\"  Configuration: {YOLO_DATASET_ROOT}/data.yaml\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n✓ Block 1 completed: Preprocessing completed succesfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScNdLpN09p9V"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 2: SETUP AND CONFIGURATION - possible rerunning from here needed after block 4 running if using cuda\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION PARAMETERS\n",
        "# ==============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for DenseFusion\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Paths - UPDATE THESE FOR YOUR SETUP\n",
        "        self.LINEMOD_ROOT = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2\"\n",
        "        self.DATA_YAML_PATH = os.path.join(LINEMOD_ROOT, 'data.yaml')\n",
        "        self.PLY_MODELS_DIR = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2/pose_models/models\"\n",
        "        self.DIAMETER_INFO_PATH = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2/pose_models/models_info.yml\"\n",
        "        self.MODELS_SAVE_DIR = \"/content/drive/MyDrive/2024-25_S2/01TXFSM - MLADL/04_3DPE_PROJECT/04_COLAB_NOTEBOOK/02_POSE_ESTIMATION/00_DENSEFUSION/02_DEV/01_FINAL_DEV_20250621/01_MODEL_STATS\"\n",
        "        self.CHECKPOINTS_DIR = \"/content/datasets/linemod/Linemod_preprocessed_yolo_2/checkpoints\"\n",
        "        self.YOLO_PROJ_NAME = \"YOLO_Linemod\"\n",
        "        self.YOLO_NAME = \"yolov11s_adam_finetuning\"\n",
        "        self.YOLO_SAVING_PATH = \"/content/\"+self.YOLO_PROJ_NAME+\"/\"+self.YOLO_NAME+\"/weights/best.pt\"\n",
        "        self.YOLO_MODEL_PATH = \"/content/YOLOv11_finetuning/weights/best.pt\" # YOLO has been moved here for dev purposes\n",
        "        self.MODELS_NAME = \"EC_500_512_MLP\" # change name to load model (for dev purpose e.g. XX_npoints_npatches_MLP/TRS)\n",
        "\n",
        "        # Device configuration\n",
        "        self.DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "        # Scale factors\n",
        "        self.MODEL_SCALE_MM_TO_M = 0.001\n",
        "        self.DEPTH_SCALE_MM_TO_M = 1000.0\n",
        "\n",
        "        # Camera intrinsics (Linemod standard)\n",
        "        self.K = np.array([\n",
        "            [572.4114, 0,        325.2611],\n",
        "            [0,        573.57043, 242.04899],\n",
        "            [0,        0,        1        ]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # List of symmetric objects\n",
        "        self.SYMMETRIC_LIST = [7,8]\n",
        "\n",
        "        # YOLO hyperparameters\n",
        "        self.YOLO_NUM_EPOCHS = 10\n",
        "        self.YOLO_IMG_SIZE = 640\n",
        "        self.YOLO_BATCH_SIZE = 10\n",
        "        self.YOLO_LR = 0.008\n",
        "\n",
        "        # Model configuration\n",
        "        self.USE_SEGMENTATION = True\n",
        "        self.MAX_EVAL_SAMPLES = 100\n",
        "        #Transformer fusion options\n",
        "        self.USE_TRANSFORMER_FUSION = False\n",
        "        self.TRANSFORMER_HEADS = 2\n",
        "        self.TRANSFORMER_LAYERS = 4\n",
        "        self.TRANSFORMER_DIM = 128\n",
        "        self.TRANSFORMER_DROPOUT = 0.1\n",
        "\n",
        "        # Model hyperparameters\n",
        "        self.NUM_POINTS = 500\n",
        "        self.PATCH_SIZE = 512\n",
        "\n",
        "        # Training hyperparameters\n",
        "        self.BATCH_SIZE = 12\n",
        "        self.NUM_EPOCHS = 15\n",
        "        self.LEARNING_RATE = 1e-4\n",
        "        self.USE_MIXED_PRECISION = True\n",
        "        self.GRADIENT_ACCUMULATION_STEPS = 2\n",
        "\n",
        "    def setup_environment(self):\n",
        "        \"\"\"Setup optimized environment for training\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cudnn.deterministic = False\n",
        "\n",
        "        # Set environment variables\n",
        "        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
        "        os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "        # Multiprocessing setup\n",
        "        import torch.multiprocessing as mp\n",
        "        try:\n",
        "            if mp.get_start_method(allow_none=True) != 'spawn':\n",
        "                mp.set_start_method('spawn', force=True)\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "\n",
        "    def verify_paths(self):\n",
        "        \"\"\"Verify all required paths exist\"\"\"\n",
        "        paths = {\n",
        "            'LINEMOD dataset': self.LINEMOD_ROOT,\n",
        "            'YOLO model': self.YOLO_MODEL_PATH,\n",
        "            'PLY models': self.PLY_MODELS_DIR,\n",
        "            'Diameter info': self.DIAMETER_INFO_PATH\n",
        "        }\n",
        "\n",
        "        all_good = True\n",
        "        for name, path in paths.items():\n",
        "            if os.path.exists(path):\n",
        "                print(f\"✓ {name}: {path}\")\n",
        "            else:\n",
        "                print(f\"✗ {name} NOT FOUND: {path}\")\n",
        "                all_good = False\n",
        "\n",
        "        return all_good\n",
        "\n",
        "    def print_config(self):\n",
        "        \"\"\"Print configuration summary\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"DENSEFUSION CONFIGURATION\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Device: {self.DEVICE}\")\n",
        "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "        print(f\"\\nTraining Configuration:\")\n",
        "        print(f\"  Batch size: {self.BATCH_SIZE}\")\n",
        "        print(f\"  Epochs: {self.NUM_EPOCHS}\")\n",
        "        print(f\"  Learning rate: {self.LEARNING_RATE}\")\n",
        "        print(f\"  Points per sample: {self.NUM_POINTS}\")\n",
        "        print(f\"  Patch size: {self.PATCH_SIZE}\")\n",
        "\n",
        "        print(f\"\\nFeatures:\")\n",
        "        print(f\"  Segmentation: {self.USE_SEGMENTATION}\")\n",
        "        print(f\"  Mixed precision: {self.USE_MIXED_PRECISION}\")\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "config.setup_environment()\n",
        "config.print_config()\n",
        "\n",
        "# Verify paths\n",
        "if not config.verify_paths():\n",
        "    print(\"\\n⚠ Please update the paths in the Config class before proceeding\")\n",
        "\n",
        "print(\"\\n✓ Block 2 completed: Configuration ready\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 3: YOLO FINE-TUNING (skip if unzipped finetuned yolov11 at start)\n",
        "# ==============================================================================\n",
        "\n",
        "# We'll fine-tune the pretrained YOLOv11s as v11 it's the current standard\n",
        "model = YOLO('yolo11s.pt')\n",
        "print(f\"Model loaded: {model.model.__class__.__name__}\")\n",
        "\n",
        "results = model.train(\n",
        "    data=config.DATA_YAML_PATH,\n",
        "    epochs=config.YOLO_NUM_EPOCHS,\n",
        "    imgsz=config.YOLO_IMG_SIZE,\n",
        "    batch=config.YOLO_BATCH_SIZE,\n",
        "    optimizer='Adam',\n",
        "    device=device,\n",
        "    lr0=config.YOLO_LR,\n",
        "    patience=3,\n",
        "    project=config.YOLO_PROJ_NAME,\n",
        "    name=config.YOLO_NAME,\n",
        "    cache='disk',\n",
        ")\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# Fine-tuned YOLO evaluation\n",
        "best_model = YOLO(config.YOLO_PATH)\n",
        "metrics = best_model.val(\n",
        "    data=config.DATA_YAML_PATH,\n",
        "    split=\"test\",\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(metrics.results_dict)\n",
        "print(\"\\n✓ Block 3 completed: YOLO finetuning complete\")"
      ],
      "metadata": {
        "id": "-IfNubsS6mOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcAvL3fv9p_l"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 4: UTILITY FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU memory\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def load_dataset_config(linemod_root):\n",
        "    \"\"\"Load dataset configuration from data.yaml\"\"\"\n",
        "    config_path = os.path.join(linemod_root, 'data.yaml')\n",
        "    with open(config_path, 'r') as f:\n",
        "        dataset_conf = yaml.safe_load(f)\n",
        "\n",
        "    # Convert relative paths to absolute\n",
        "    for split_key in ['train', 'val', 'test', 'depth_train', 'depth_val', 'depth_test']:\n",
        "        if split_key in dataset_conf and isinstance(dataset_conf[split_key], str):\n",
        "            if not os.path.isabs(dataset_conf[split_key]):\n",
        "                dataset_conf[split_key] = os.path.join(linemod_root, dataset_conf[split_key])\n",
        "\n",
        "    return dataset_conf\n",
        "\n",
        "def load_model_diameters(diameter_yml_path):\n",
        "    \"\"\"Load object model diameters from YAML file\"\"\"\n",
        "    with open(diameter_yml_path, 'r') as f:\n",
        "        diameter_data = yaml.safe_load(f)\n",
        "\n",
        "    model_diameters = {}\n",
        "    for class_id, info in diameter_data.items():\n",
        "        if isinstance(info, dict) and 'diameter' in info:\n",
        "            internal_class_id = int(class_id) - 1  # Convert to 0-based\n",
        "            model_diameters[internal_class_id] = float(info['diameter']) * config.MODEL_SCALE_MM_TO_M\n",
        "\n",
        "    return model_diameters\n",
        "\n",
        "def load_yolo_model(model_path):\n",
        "    \"\"\"Load and validate YOLO model\"\"\"\n",
        "    try:\n",
        "        yolo_model = YOLO(model_path)\n",
        "        # Test with dummy image\n",
        "        test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
        "        results = yolo_model(test_image, verbose=False)\n",
        "        print(f\"✓ YOLO model loaded successfully\")\n",
        "        return yolo_model\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load YOLO model: {e}\")\n",
        "        return None\n",
        "\n",
        "def decompose_pose_numpy(pose_numpy):\n",
        "    \"\"\"Decompose 7D pose [tx, ty, tz, qw, qx, qy, qz] into rotation matrix and translation\"\"\"\n",
        "    t = pose_numpy[:3]\n",
        "    q_wxyz = pose_numpy[3:]\n",
        "\n",
        "    # Normalize quaternion\n",
        "    norm_q = np.linalg.norm(q_wxyz)\n",
        "    if norm_q < 1e-6:\n",
        "        return np.identity(3), t\n",
        "    q_wxyz = q_wxyz / norm_q\n",
        "\n",
        "    # Convert to rotation matrix (scipy expects [x, y, z, w])\n",
        "    rot = R.from_quat([q_wxyz[1], q_wxyz[2], q_wxyz[3], q_wxyz[0]]).as_matrix()\n",
        "    return rot, t\n",
        "\n",
        "def compute_add_metric(pred_pose_numpy, gt_pose_numpy, model_vertices):\n",
        "    \"\"\"Compute ADD (Average Distance) metric for pose estimation\"\"\"\n",
        "    if model_vertices is None or model_vertices.shape[0] == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    try:\n",
        "        R_pred, t_pred = decompose_pose_numpy(pred_pose_numpy)\n",
        "        R_gt, t_gt = decompose_pose_numpy(gt_pose_numpy)\n",
        "\n",
        "        # Scale model vertices from mm to meters\n",
        "        model_points_meters = model_vertices * config.MODEL_SCALE_MM_TO_M\n",
        "\n",
        "        # Transform model points\n",
        "        pred_transformed = (R_pred @ model_points_meters.T).T + t_pred\n",
        "        gt_transformed = (R_gt @ model_points_meters.T).T + t_gt\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = np.linalg.norm(pred_transformed - gt_transformed, axis=1)\n",
        "        return np.mean(distances)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ADD computation error: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "def compute_add_s_metric(pred_pose_numpy, gt_pose_numpy, model_vertices):\n",
        "    \"\"\"Compute ADD-S (symmetric version of ADD) for pose estimation.\"\"\"\n",
        "    if model_vertices is None or model_vertices.shape[0] == 0:\n",
        "        print(\"ERROR: INVALID 3D MODEL\")\n",
        "        return float('inf')\n",
        "\n",
        "    try:\n",
        "        # Decompose predicted and GT poses\n",
        "        R_pred, t_pred = decompose_pose_numpy(pred_pose_numpy)\n",
        "        R_gt, t_gt = decompose_pose_numpy(gt_pose_numpy)\n",
        "\n",
        "        # Scale model vertices from mm to meters\n",
        "        model_points_meters = model_vertices * config.MODEL_SCALE_MM_TO_M\n",
        "\n",
        "        # Transform model points\n",
        "        pred_transformed = (R_pred @ model_points_meters.T).T + t_pred  # [M, 3]\n",
        "        gt_transformed = (R_gt @ model_points_meters.T).T + t_gt        # [M, 3]\n",
        "\n",
        "        # Build KD-tree for GT transformed points\n",
        "        gt_kdtree = cKDTree(gt_transformed)\n",
        "\n",
        "        # Find nearest GT point for each predicted point\n",
        "        distances, _ = gt_kdtree.query(pred_transformed, k=1)\n",
        "\n",
        "        # Return mean of nearest distances\n",
        "        return np.mean(distances)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ADD-S computation error: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "def compute_add_metrics_with_thresholds(pred_pose, gt_pose, class_id, sym_list, model_vertices, diameter=None):\n",
        "    \"\"\"Compute ADD metric with various success thresholds\"\"\"\n",
        "\n",
        "    if class_id in sym_list:\n",
        "        add_value = compute_add_s_metric(pred_pose, gt_pose, model_vertices)\n",
        "    else:\n",
        "        add_value = compute_add_metric(pred_pose, gt_pose, model_vertices)\n",
        "\n",
        "    results = {\n",
        "        \"add_value\": add_value,\n",
        "        \"add_success_2cm\": add_value < 0.02,\n",
        "        \"add_success_5cm\": add_value < 0.05,\n",
        "        \"add_success_10cm\": add_value < 0.10,\n",
        "    }\n",
        "\n",
        "    if diameter is not None and diameter > 0:\n",
        "        results.update({\n",
        "            \"diameter\": diameter,\n",
        "            \"add_success_5p\": add_value < (0.05 * diameter),\n",
        "            \"add_success_10p\": add_value < (0.10 * diameter),\n",
        "            \"add_success_20p\": add_value < (0.20 * diameter),\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def compute_rotation_difference_degrees(pred_pose, gt_pose):\n",
        "    \"\"\"Compute rotational difference in degrees between poses\"\"\"\n",
        "    try:\n",
        "        pred_quat = pred_pose[3:] / np.linalg.norm(pred_pose[3:])\n",
        "        gt_quat = gt_pose[3:] / np.linalg.norm(gt_pose[3:])\n",
        "\n",
        "        # Convert to scipy format [qx, qy, qz, qw]\n",
        "        pred_quat_scipy = [pred_quat[1], pred_quat[2], pred_quat[3], pred_quat[0]]\n",
        "        gt_quat_scipy = [gt_quat[1], gt_quat[2], gt_quat[3], gt_quat[0]]\n",
        "\n",
        "        pred_rot = R.from_quat(pred_quat_scipy)\n",
        "        gt_rot = R.from_quat(gt_quat_scipy)\n",
        "\n",
        "        # Overall angular difference\n",
        "        relative_rot = pred_rot * gt_rot.inv()\n",
        "        overall_angle_deg = np.degrees(relative_rot.magnitude())\n",
        "\n",
        "        # Per-axis differences\n",
        "        pred_euler = pred_rot.as_euler('xyz', degrees=True)\n",
        "        gt_euler = gt_rot.as_euler('xyz', degrees=True)\n",
        "\n",
        "        diff_x = min(abs(pred_euler[0] - gt_euler[0]), 360 - abs(pred_euler[0] - gt_euler[0]))\n",
        "        diff_y = min(abs(pred_euler[1] - gt_euler[1]), 360 - abs(pred_euler[1] - gt_euler[1]))\n",
        "        diff_z = min(abs(pred_euler[2] - gt_euler[2]), 360 - abs(pred_euler[2] - gt_euler[2]))\n",
        "\n",
        "        return {\n",
        "            'overall': overall_angle_deg,\n",
        "            'x_axis': diff_x,\n",
        "            'y_axis': diff_y,\n",
        "            'z_axis': diff_z,\n",
        "            'pred_euler': pred_euler,\n",
        "            'gt_euler': gt_euler\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'overall': float('inf'), 'x_axis': 0, 'y_axis': 0, 'z_axis': 0}\n",
        "\n",
        "def convert_yolo_bbox_to_pixel(bbox_normalized, image_width, image_height):\n",
        "    \"\"\"Convert YOLO normalized bbox to pixel coordinates\"\"\"\n",
        "    xc_n, yc_n, w_n, h_n = bbox_normalized\n",
        "\n",
        "    xc_px = xc_n * image_width\n",
        "    yc_px = yc_n * image_height\n",
        "    w_px = w_n * image_width\n",
        "    h_px = h_n * image_height\n",
        "\n",
        "    x1 = max(0, int(xc_px - w_px / 2))\n",
        "    y1 = max(0, int(yc_px - h_px / 2))\n",
        "    x2 = min(image_width, int(xc_px + w_px / 2))\n",
        "    y2 = min(image_height, int(yc_px + h_px / 2))\n",
        "\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create necessary directories\"\"\"\n",
        "    directories = [config.MODELS_SAVE_DIR, config.CHECKPOINTS_DIR]\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"✓ Block 4 completed: Utility functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsjfQOrB9qCG"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 5: SEGMENTATION MODULE\n",
        "# ==============================================================================\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "class DenseFusionSegmentationModule:\n",
        "    \"\"\"Segmentation module using Mask R-CNN for instance segmentation\"\"\"\n",
        "\n",
        "    def __init__(self, confidence_threshold=0.5):\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.device = config.DEVICE\n",
        "        self.model = None\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.stats = {\n",
        "            'total_calls': 0,\n",
        "            'successful_segmentations': 0,\n",
        "            'bbox_fallbacks': 0\n",
        "        }\n",
        "\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize Mask R-CNN model\"\"\"\n",
        "        try:\n",
        "            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
        "                weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
        "            )\n",
        "            self.model.eval()\n",
        "            self.model = self.model.to(self.device)\n",
        "            print(\"✓ Mask R-CNN model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to initialize Mask R-CNN: {e}\")\n",
        "            self.model = None\n",
        "\n",
        "    def refine_detection(self, rgb_image, bbox_pixel, class_id=None):\n",
        "        \"\"\"Refine YOLO detection using Mask R-CNN segmentation\"\"\"\n",
        "        self.stats['total_calls'] += 1\n",
        "\n",
        "        if self.model is None:\n",
        "            return self._bbox_to_mask(rgb_image.shape[:2], bbox_pixel), {\n",
        "                'source': 'bbox_fallback_no_model'\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Prepare image\n",
        "            if isinstance(rgb_image, np.ndarray):\n",
        "                if rgb_image.dtype != np.uint8:\n",
        "                    rgb_image = (rgb_image * 255).astype(np.uint8)\n",
        "                pil_image = Image.fromarray(rgb_image)\n",
        "                image_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Run inference\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(image_tensor)\n",
        "\n",
        "            if len(predictions) == 0 or len(predictions[0]['masks']) == 0:\n",
        "                self.stats['bbox_fallbacks'] += 1\n",
        "                return self._bbox_to_mask(rgb_image.shape[:2], bbox_pixel), {\n",
        "                    'source': 'bbox_fallback_no_detections'\n",
        "                }\n",
        "\n",
        "            # Find best mask\n",
        "            best_mask, best_info = self._find_best_mask(\n",
        "                predictions[0], bbox_pixel, rgb_image.shape[:2]\n",
        "            )\n",
        "\n",
        "            if best_mask is not None:\n",
        "                self.stats['successful_segmentations'] += 1\n",
        "                return best_mask, best_info\n",
        "            else:\n",
        "                self.stats['bbox_fallbacks'] += 1\n",
        "                return self._bbox_to_mask(rgb_image.shape[:2], bbox_pixel), {\n",
        "                    'source': 'bbox_fallback_low_overlap'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.stats['bbox_fallbacks'] += 1\n",
        "            return self._bbox_to_mask(rgb_image.shape[:2], bbox_pixel), {\n",
        "                'source': 'bbox_fallback_error',\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _find_best_mask(self, prediction, yolo_bbox, image_shape):\n",
        "        \"\"\"Find the best mask that overlaps with YOLO detection\"\"\"\n",
        "        x1, y1, x2, y2 = map(int, yolo_bbox)\n",
        "        yolo_area = max(1, (x2 - x1) * (y2 - y1))\n",
        "\n",
        "        best_mask = None\n",
        "        best_score = 0\n",
        "        best_info = {'source': 'bbox_fallback'}\n",
        "\n",
        "        masks = prediction['masks']\n",
        "        scores = prediction['scores']\n",
        "        boxes = prediction['boxes']\n",
        "\n",
        "        for mask_tensor, score, box in zip(masks, scores, boxes):\n",
        "            if score < self.confidence_threshold:\n",
        "                continue\n",
        "\n",
        "            # Convert mask to numpy\n",
        "            mask_np = mask_tensor.squeeze().cpu().numpy()\n",
        "            if mask_np.shape != image_shape:\n",
        "                mask_np = cv2.resize(mask_np, (image_shape[1], image_shape[0]),\n",
        "                                   interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            mask_binary = (mask_np > 0.5).astype(np.uint8)\n",
        "\n",
        "            # Calculate overlap with YOLO bbox\n",
        "            mask_in_bbox = mask_binary[y1:y2, x1:x2]\n",
        "            overlap_area = np.sum(mask_in_bbox)\n",
        "            overlap_ratio = overlap_area / yolo_area if yolo_area > 0 else 0\n",
        "\n",
        "            # Calculate IoU between predicted box and YOLO box\n",
        "            pred_x1, pred_y1, pred_x2, pred_y2 = box.cpu().numpy()\n",
        "            inter_x1 = max(x1, pred_x1)\n",
        "            inter_y1 = max(y1, pred_y1)\n",
        "            inter_x2 = min(x2, pred_x2)\n",
        "            inter_y2 = min(y2, pred_y2)\n",
        "\n",
        "            iou = 0.0\n",
        "            if inter_x2 > inter_x1 and inter_y2 > inter_y1:\n",
        "                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
        "                pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
        "                union_area = yolo_area + pred_area - inter_area\n",
        "                iou = inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "            # Combined score\n",
        "            combined_score = float(score) * 0.4 + overlap_ratio * 0.3 + iou * 0.3\n",
        "\n",
        "            if combined_score > best_score and overlap_ratio > 0.1:\n",
        "                best_score = combined_score\n",
        "                best_mask = mask_binary\n",
        "                best_info = {\n",
        "                    'source': 'mask_rcnn',\n",
        "                    'confidence': float(score),\n",
        "                    'overlap': overlap_ratio,\n",
        "                    'iou': iou\n",
        "                }\n",
        "\n",
        "        return best_mask, best_info\n",
        "\n",
        "    def _bbox_to_mask(self, image_shape, bbox_pixel):\n",
        "        \"\"\"Fallback: create mask from bounding box\"\"\"\n",
        "        mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "        x1, y1, x2, y2 = map(int, bbox_pixel)\n",
        "\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(image_shape[1], x2), min(image_shape[0], y2)\n",
        "\n",
        "        mask[y1:y2, x1:x2] = 1\n",
        "        return mask\n",
        "\n",
        "    def print_stats(self):\n",
        "        \"\"\"Print segmentation statistics\"\"\"\n",
        "        if self.stats['total_calls'] > 0:\n",
        "            success_rate = self.stats['successful_segmentations'] / self.stats['total_calls']\n",
        "            print(f\"Segmentation Statistics:\")\n",
        "            print(f\"  Total calls: {self.stats['total_calls']}\")\n",
        "            print(f\"  Success rate: {success_rate:.2%}\")\n",
        "\n",
        "# Initialize segmentation module\n",
        "segmentation_module = None\n",
        "if config.USE_SEGMENTATION:\n",
        "    segmentation_module = DenseFusionSegmentationModule()\n",
        "\n",
        "print(\"✓ Block 5 completed: Segmentation module ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXBjbdSq9qEY"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 6: DENSEFUSION MODEL ARCHITECTURE\n",
        "# ==============================================================================\n",
        "\n",
        "class RGBFeatureExtractor(nn.Module):\n",
        "    \"\"\"CNN for feature extraction from RGB images\"\"\"\n",
        "    def __init__(self, d_rgb = 32):\n",
        "        super(RGBFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Convolutional backbone\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Feature pyramid pooling\n",
        "        self.psp = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.final_conv = nn.Conv2d(512, d_rgb, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "        x = self.psp(x)\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "class PointNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"PointNet-style feature extractor for point clouds\"\"\"\n",
        "    def __init__(self, d_geo = 32):\n",
        "        super(PointNetFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Point-wise MLPs\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # Global feature\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, d_geo)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, N, 3] -> [B, 3, N]\n",
        "        x = x.transpose(2, 1)\n",
        "\n",
        "        # Point-wise convolutions\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Global max pooling\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        # MLP for global features\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerFuser(nn.Module):\n",
        "    \"\"\"Fuses a global RGB and a global Point Cloud feature vector using a Transformer.\"\"\"\n",
        "    def __init__(self, rgb_feature_dim, geo_feature_dim, embed_dim=config.TRANSFORMER_DIM, num_heads=config.TRANSFORMER_HEADS, num_layers=config.TRANSFORMER_LAYERS, dropout=config.TRANSFORMER_DROPOUT):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Linear layers to project the 32-dim features into a common embedding space\n",
        "        self.rgb_proj = nn.Linear(rgb_feature_dim, embed_dim)\n",
        "        self.point_proj = nn.Linear(geo_feature_dim, embed_dim)\n",
        "\n",
        "        # A learnable token that will act as the final fused representation\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Learnable positional embeddings for the 3 tokens: [CLS, RGB, PointCloud]\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 3, embed_dim))\n",
        "\n",
        "        # The Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # The final layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, rgb_features, point_features):\n",
        "        # rgb_features: [B, 32], point_features: [B, 32]\n",
        "        batch_size = rgb_features.shape[0]\n",
        "\n",
        "        # 1. Project features into the embedding dimension\n",
        "        rgb_embed = self.rgb_proj(rgb_features).unsqueeze(1)    # Shape: [B, 1, 128]\n",
        "        point_embed = self.point_proj(point_features).unsqueeze(1)  # Shape: [B, 1, 128]\n",
        "\n",
        "        # 2. Prepare the sequence: [CLS, RGB, PointCloud]\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # Shape: [B, 1, 128]\n",
        "        token_sequence = torch.cat((cls_tokens, rgb_embed, point_embed), dim=1) # Shape: [B, 3, 128]\n",
        "\n",
        "        # 3. Add positional embeddings\n",
        "        token_sequence = token_sequence + self.pos_embedding\n",
        "\n",
        "        # 4. Pass through the transformer\n",
        "        fused_sequence = self.transformer(token_sequence) # Shape: [B, 3, 128]\n",
        "        fused_sequence = self.norm(fused_sequence)\n",
        "\n",
        "        # 5. Extract the output of the [CLS] token as our final fused vector\n",
        "        cls_output = fused_sequence[:, 0] # Shape: [B, 128]\n",
        "\n",
        "        return cls_output\n",
        "\n",
        "\n",
        "\n",
        "class DenseFusionNetwork(nn.Module):\n",
        "    \"\"\"Dense fusion network combining RGB and point cloud features\"\"\"\n",
        "    def __init__(self, num_objects=13, use_transformer=config.USE_TRANSFORMER_FUSION, d_rgb = 32, d_geo = 32):\n",
        "        super(DenseFusionNetwork, self).__init__()\n",
        "        self.num_objects = num_objects\n",
        "\n",
        "        #TRANSFORMER USE\n",
        "        self.use_transformer = use_transformer\n",
        "\n",
        "        # Feature extractors\n",
        "        self.rgb_extractor = RGBFeatureExtractor(d_rgb)\n",
        "        self.point_extractor = PointNetFeatureExtractor(d_geo)\n",
        "\n",
        "        if self.use_transformer:\n",
        "            self.transformer_fuser = TransformerFuser(rgb_feature_dim=d_rgb, geo_feature_dim=d_geo)\n",
        "\n",
        "            # Pose regression head\n",
        "            self.pose_fc1 = nn.Linear(128, 512)\n",
        "            self.pose_fc2 = nn.Linear(512, 256)\n",
        "            self.pose_fc3 = nn.Linear(256, 7)\n",
        "\n",
        "            # Confidence estimation head\n",
        "            self.conf_fc1 = nn.Linear(128, 256)\n",
        "            self.conf_fc2 = nn.Linear(256, 64)\n",
        "            self.conf_fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        else:\n",
        "            self.fusion_head = nn.Sequential(\n",
        "                nn.Linear(d_rgb + d_geo, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 1024),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "            # Pose regression head\n",
        "            self.pose_fc1 = nn.Linear(1024, 512)\n",
        "            self.pose_fc2 = nn.Linear(512, 256)\n",
        "            self.pose_fc3 = nn.Linear(256, 7)  # [tx, ty, tz, qw, qx, qy, qz] - translation + quaternion\n",
        "\n",
        "            # Confidence estimation head\n",
        "            self.conf_fc1 = nn.Linear(1024, 256)\n",
        "            self.conf_fc2 = nn.Linear(256, 64)\n",
        "            self.conf_fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self._initialize_weights()\n",
        "\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Conv1d)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    if m.out_features == 7:  # Pose prediction layer\n",
        "                        nn.init.constant_(m.bias, 0)\n",
        "                        with torch.no_grad():\n",
        "                            m.bias[:3] = torch.tensor([0.0, 0.0, 0.3])  # translation\n",
        "                            m.bias[3:] = torch.tensor([1.0, 0.0, 0.0, 0.0])  # quaternion\n",
        "                    else:\n",
        "                        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, rgb, points):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        batch_size = rgb.size(0)\n",
        "        num_points = points.size(1)\n",
        "\n",
        "        # Extract features\n",
        "        rgb_features = self.rgb_extractor(rgb).view(batch_size, -1)      # Shape: [B, 32]\n",
        "        point_features = self.point_extractor(points)  # [B, 32]\n",
        "\n",
        "\n",
        "        # # CONDITIONAL FUSION\n",
        "        if self.use_transformer:\n",
        "              x = self.transformer_fuser(rgb_features, point_features) # Shape: [B, 128]\n",
        "        else:\n",
        "              combined_features = torch.cat([rgb_features, point_features], dim=1) # Shape: [B, 64]\n",
        "              x = self.fusion_head(combined_features)                           # Shape: [B, 1024]\n",
        "\n",
        "        # Pose prediction\n",
        "        pose_x = F.relu(self.pose_fc1(x))\n",
        "        pose_x = self.dropout(pose_x)\n",
        "        pose_x = F.relu(self.pose_fc2(pose_x))\n",
        "        pose_x = self.dropout(pose_x)\n",
        "        pose = self.pose_fc3(pose_x)\n",
        "\n",
        "        # Normalize quaternion\n",
        "        translation = pose[:, :3]\n",
        "        quaternion = pose[:, 3:]\n",
        "        quaternion = F.normalize(quaternion, p=2, dim=1)\n",
        "        pose = torch.cat([translation, quaternion], dim=1)\n",
        "\n",
        "        # Confidence prediction\n",
        "        conf_x = F.relu(self.conf_fc1(x))\n",
        "        conf_x = self.dropout(conf_x)\n",
        "        conf_x = F.relu(self.conf_fc2(conf_x))\n",
        "        confidence = self.conf_fc3(conf_x)\n",
        "\n",
        "        return pose, confidence\n",
        "\n",
        "\n",
        "#######\n",
        "\n",
        "def test_model_architecture():\n",
        "    \"\"\"Test the Global Fusion architecture\"\"\"\n",
        "    try:\n",
        "        print(\"\\nTesting Global Fusion version...\")\n",
        "\n",
        "        # Create the model (no 'use_transformer' argument needed)\n",
        "        model = DenseFusionNetwork(num_objects=13)\n",
        "        model = model.to(config.DEVICE)\n",
        "\n",
        "        # --- Test with dummy data (this part is the same) ---\n",
        "        batch_size = 2\n",
        "        # Assuming config has PATCH_SIZE and NUM_POINTS defined\n",
        "        test_rgb = torch.randn(batch_size, 3, config.PATCH_SIZE, config.PATCH_SIZE).to(config.DEVICE)\n",
        "        test_points = torch.randn(batch_size, config.NUM_POINTS, 3).to(config.DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_pose, pred_conf = model(test_rgb, test_points)\n",
        "\n",
        "        print(f\"  ✓ Output pose shape: {pred_pose.shape}\")\n",
        "        print(f\"  ✓ Output confidence shape: {pred_conf.shape}\")\n",
        "        print(f\"  ✓ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Model test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test the architecture\n",
        "test_model = test_model_architecture()\n",
        "print(\"\\n✓ Block 6 completed: DenseFusion architecture ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyH6mbONDAMZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 7: LOSS FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "class DenseFusionLoss(nn.Module):\n",
        "    \"\"\"Complete loss function for DenseFusion training\"\"\"\n",
        "\n",
        "    def __init__(self, object_models, sym_list, add_weight=1.0, conf_weight=0.1):\n",
        "        super(DenseFusionLoss, self).__init__()\n",
        "        self.object_models = object_models\n",
        "        self.add_weight = add_weight  # Increased to balance scales\n",
        "        self.conf_weight = conf_weight  # Reduced to prevent domination\n",
        "        self.conf_loss = nn.BCEWithLogitsLoss()\n",
        "        self.reset_stats()\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset statistics for tracking\"\"\"\n",
        "        self.stats = {\n",
        "            'total_samples': 0,\n",
        "            'add_computed': 0,\n",
        "            'pose_fallback': 0,\n",
        "            'avg_add_error': 0.0\n",
        "        }\n",
        "\n",
        "    def quaternion_to_matrix(self, q):\n",
        "        \"\"\"Safe, batched quaternion to matrix conversion\"\"\"\n",
        "        if q.dim() == 1:\n",
        "            q = q.unsqueeze(0)\n",
        "\n",
        "        q = F.normalize(q, p=2, dim=-1)\n",
        "        w, x, y, z = q.unbind(-1)\n",
        "\n",
        "        xx, yy, zz = x*x, y*y, z*z\n",
        "        xy, xz, yz = x*y, x*z, y*z\n",
        "        wx, wy, wz = w*x, w*y, w*z\n",
        "\n",
        "        matrix = torch.stack([\n",
        "            1 - 2*(yy + zz), 2*(xy - wz), 2*(xz + wy),\n",
        "            2*(xy + wz), 1 - 2*(xx + zz), 2*(yz - wx),\n",
        "            2*(xz - wy), 2*(yz + wx), 1 - 2*(xx + yy)\n",
        "        ], dim=-1).view(q.size(0), 3, 3)\n",
        "\n",
        "        return matrix.squeeze(0) if matrix.size(0) == 1 else matrix\n",
        "\n",
        "    def compute_add_loss(self, pred_pose, gt_pose, class_id):\n",
        "        \"\"\"Compute ADD loss for a single sample\"\"\"\n",
        "        if class_id not in self.object_models:\n",
        "            return torch.tensor(0.0, device=pred_pose.device)\n",
        "\n",
        "        vertices = torch.tensor(\n",
        "            self.object_models[class_id]['vertices_raw'],\n",
        "            device=pred_pose.device,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        if vertices.shape[0] < 10:\n",
        "              return self._pose_distance_loss(pred_pose, gt_pose)\n",
        "        # Extract components\n",
        "        t_pred = pred_pose[:3]\n",
        "        q_pred = pred_pose[3:]\n",
        "        t_gt = gt_pose[:3]\n",
        "        q_gt = gt_pose[3:]\n",
        "\n",
        "        # Convert to rotation matrices\n",
        "        R_pred = self.quaternion_to_matrix(q_pred)\n",
        "        R_gt = self.quaternion_to_matrix(q_gt)\n",
        "\n",
        "        # Transform model points\n",
        "        pred_pts = vertices @ R_pred.T + t_pred\n",
        "        gt_pts = vertices @ R_gt.T + t_gt\n",
        "\n",
        "        # Calculate distances (scale to cm)\n",
        "        dists = torch.norm(pred_pts - gt_pts, p=2, dim=1)\n",
        "        add_loss = dists.mean() # Convert to cm scale\n",
        "        if add_loss == float('inf') or torch.isnan(add_loss.detach()):\n",
        "            print(\"ERROR: CAN'T COMPUTE ADD LOSS\")\n",
        "        # Update statistics\n",
        "        self.stats['add_computed'] += 1\n",
        "        self.stats['avg_add_error'] += add_loss.item()\n",
        "\n",
        "        return add_loss\n",
        "\n",
        "    def compute_add_s_loss(self, pred_pose, gt_pose, class_id):\n",
        "        \"\"\"Compute ADD-S loss for a single sample\"\"\"\n",
        "        if class_id not in self.object_models:\n",
        "            return torch.tensor(0.0, device=pred_pose.device)\n",
        "\n",
        "        vertices = torch.tensor(\n",
        "            self.object_models[class_id]['vertices_raw'],\n",
        "            device=pred_pose.device,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        if vertices.shape[0] < 10:\n",
        "            return self._pose_distance_loss(pred_pose, gt_pose)\n",
        "\n",
        "        # Extract translation and quaternion\n",
        "        t_pred = pred_pose[:3]\n",
        "        q_pred = pred_pose[3:]\n",
        "        t_gt = gt_pose[:3]\n",
        "        q_gt = gt_pose[3:]\n",
        "\n",
        "        # Convert quaternion to rotation matrices\n",
        "        R_pred = self.quaternion_to_matrix(q_pred)\n",
        "        R_gt = self.quaternion_to_matrix(q_gt)\n",
        "\n",
        "        # Transform model vertices to world coordinates\n",
        "        pred_pts = vertices @ R_pred.T + t_pred\n",
        "        gt_pts = vertices @ R_gt.T + t_gt\n",
        "\n",
        "        # ADD-S: Closest point distance for symmetric objects\n",
        "        dists = torch.cdist(pred_pts.unsqueeze(0), gt_pts.unsqueeze(0)).squeeze(0)\n",
        "        add_loss = dists.min(dim=1)[0].mean()\n",
        "\n",
        "        if add_loss == float('inf') or torch.isnan(add_loss.detach()):\n",
        "            print(\"ERROR: CAN'T COMPUTE ADD-S LOSS\")\n",
        "\n",
        "        self.stats['add_computed'] += 1\n",
        "        self.stats['avg_add_error'] += add_loss.item()\n",
        "\n",
        "        return add_loss\n",
        "\n",
        "    def forward(self, pred_poses, gt_poses, pred_confidences, class_ids):\n",
        "        \"\"\"Compute total loss\"\"\"\n",
        "        batch_size = pred_poses.size(0)\n",
        "        self.stats['total_samples'] += batch_size\n",
        "\n",
        "        total_add_loss = 0.0\n",
        "        valid_samples = 0\n",
        "\n",
        "        # Compute ADD loss for each sample\n",
        "        for i in range(batch_size):\n",
        "            class_id = class_ids[i].item()\n",
        "            if class_id in self.sym_list:\n",
        "                add_loss_val = self.compute_add_s_loss(pred_poses[i], gt_poses[i], class_id)\n",
        "            else:\n",
        "                add_loss_val = self.compute_add_loss(pred_poses[i], gt_poses[i], class_id)\n",
        "            total_add_loss += add_loss_val\n",
        "            valid_samples += 1\n",
        "\n",
        "        avg_add_loss = total_add_loss / batch_size\n",
        "\n",
        "        # Confidence loss\n",
        "        conf_targets = torch.ones_like(pred_confidences)\n",
        "        conf_loss = self.conf_loss(pred_confidences, conf_targets)\n",
        "        pose_reg_loss = torch.tensor(0.0, device=pred_poses.device)\n",
        "        for i in range(batch_size):\n",
        "            trans_magnitude = torch.norm(pred_poses[i, :3])\n",
        "            if trans_magnitude > 2.0:\n",
        "                pose_reg_loss += (trans_magnitude - 2.0) ** 2\n",
        "        pose_reg_loss = pose_reg_loss / batch_size\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = (self.add_weight * avg_add_loss +\n",
        "                     self.conf_weight * conf_loss +\n",
        "                     0.1 * pose_reg_loss)\n",
        "        loss_dict = {\n",
        "            'total_loss': total_loss.item(),\n",
        "            'add_loss': avg_add_loss.item(),\n",
        "            'conf_loss': conf_loss.item(),\n",
        "            'valid_samples': valid_samples\n",
        "        }\n",
        "\n",
        "        return total_loss, loss_dict\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get current statistics\"\"\"\n",
        "        if self.stats['total_samples'] > 0:\n",
        "            return {\n",
        "                'total_samples': self.stats['total_samples'],\n",
        "                'add_computed_ratio': self.stats['add_computed'] / self.stats['total_samples'],\n",
        "                'pose_fallback_ratio': self.stats['pose_fallback'] / self.stats['total_samples'],\n",
        "                'avg_add_error': self.stats['avg_add_error'] / max(self.stats['add_computed'], 1)\n",
        "            }\n",
        "        return self.stats\n",
        "\n",
        "def test_loss_function():\n",
        "    \"\"\"Test the DenseFusionLoss with raw & GT vertices\"\"\"\n",
        "    try:\n",
        "        # 1) Create dummy object models with raw & GT vertices\n",
        "        dummy_models = {}\n",
        "        for i in range(3):\n",
        "            # 100 random points in object frame (in meters)\n",
        "            verts_raw = (np.random.randn(100, 3) * 0.05).astype(np.float32)\n",
        "            # random GT rotation & translation\n",
        "            axis = np.random.randn(3)\n",
        "            angle = np.random.rand() * 2 * np.pi\n",
        "            R_gt = R.from_rotvec(axis/np.linalg.norm(axis)*angle).as_matrix().astype(np.float32)\n",
        "            t_gt = (np.random.randn(3) * 0.1).astype(np.float32)  # up to ±10cm\n",
        "            verts_gt = verts_raw @ R_gt.T + t_gt[np.newaxis, :]\n",
        "\n",
        "            dummy_models[i] = {\n",
        "                'vertices_raw': verts_raw,  # Use raw vertices for model\n",
        "            }\n",
        "\n",
        "        loss_fn = DenseFusionLoss(dummy_models, config.SYMMETRIC_LIST)\n",
        "\n",
        "        # 2) Generate test inputs\n",
        "        batch_size = 2\n",
        "        # Predicted poses: [tx,ty,tz,qw,qx,qy,qz]\n",
        "        test_pred_poses = torch.randn(batch_size, 7, requires_grad=True)\n",
        "        test_gt_poses = torch.randn(batch_size, 7, requires_grad=True)\n",
        "\n",
        "        # Normalize quaternion part\n",
        "        with torch.no_grad():\n",
        "            for i in range(batch_size):\n",
        "                q = test_pred_poses.data[i, 3:]\n",
        "                test_pred_poses.data[i, 3:] = F.normalize(q, p=2, dim=0)\n",
        "                q = test_gt_poses.data[i, 3:]\n",
        "                test_gt_poses.data[i, 3:] = F.normalize(q, p=2, dim=0)\n",
        "\n",
        "        # Predicted confidences\n",
        "        test_pred_confs = torch.randn(batch_size, 1, requires_grad=True)\n",
        "        # Random class ids in [0,2]\n",
        "        test_class_ids = torch.randint(0, 3, (batch_size,))\n",
        "\n",
        "        # 3) Call loss with exactly the three required args\n",
        "        total_loss, loss_dict = loss_fn(test_pred_poses, test_gt_poses, test_pred_confs, test_class_ids)\n",
        "\n",
        "        # 4) Backward to verify gradients flow\n",
        "        total_loss.backward()\n",
        "\n",
        "        print(f\"✓ Loss function test successful:\")\n",
        "        print(f\"  Total loss:   {loss_dict['total_loss']:.6f}\")\n",
        "        print(f\"  ADD loss:     {loss_dict['add_loss']:.6f}\")\n",
        "        print(f\"  Conf loss:    {loss_dict['conf_loss']:.6f}\")\n",
        "\n",
        "        return loss_fn\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Loss function test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run the test\n",
        "test_loss = test_loss_function()\n",
        "print(\"\\n✓ Block 7 completed: Loss functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gfGgxmh9qLY"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 8: DATASET CLASS\n",
        "# ==============================================================================\n",
        "\n",
        "class DenseFusionDataset(Dataset):\n",
        "    \"\"\"Dataset for DenseFusion training and evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, data_config, split='train', num_points=None,\n",
        "                patch_size=None, use_segmentation=config.USE_SEGMENTATION):\n",
        "        self.data_config = data_config\n",
        "        self.split = split\n",
        "        self.num_points = num_points or config.NUM_POINTS\n",
        "        self.patch_size = patch_size or config.PATCH_SIZE\n",
        "        self.use_segmentation = use_segmentation\n",
        "\n",
        "        # Dataset paths\n",
        "        self.rgb_dir = data_config[split]\n",
        "        self.depth_dir = data_config.get(f'depth_{split}')\n",
        "\n",
        "        if not os.path.exists(self.rgb_dir):\n",
        "            raise FileNotFoundError(f\"RGB directory not found: {self.rgb_dir}\")\n",
        "\n",
        "        # Load image paths\n",
        "        rgb_extensions = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
        "        all_rgb_paths = []\n",
        "        for ext in rgb_extensions:\n",
        "            all_rgb_paths.extend(glob.glob(os.path.join(self.rgb_dir, ext)))\n",
        "\n",
        "        self.rgb_paths = sorted(all_rgb_paths)\n",
        "        print(f\"Dataset '{split}': {len(self.rgb_paths)} images\")\n",
        "        self.object_models = {}\n",
        "        self._load_raw_models(data_config.get('names', []))\n",
        "        # Load 3D object models\n",
        "\n",
        "    def _load_raw_models(self, object_names):\n",
        "        \"\"\"Load 3D object models from PLY files\"\"\"\n",
        "        self.object_models = {}\n",
        "\n",
        "        for obj_idx, obj_name in enumerate(object_names):\n",
        "            vertices = None\n",
        "\n",
        "            if os.path.exists(config.PLY_MODELS_DIR):\n",
        "                ply_candidates = [\n",
        "                    os.path.join(config.PLY_MODELS_DIR, f\"obj_{obj_idx+1:02d}.ply\"),\n",
        "                    os.path.join(config.PLY_MODELS_DIR, f\"obj_{obj_idx+1}.ply\"),\n",
        "                    os.path.join(config.PLY_MODELS_DIR, f\"{obj_name}.ply\")\n",
        "                ]\n",
        "\n",
        "                for ply_path in ply_candidates:\n",
        "                    if os.path.exists(ply_path):\n",
        "                        try:\n",
        "                            mesh = trimesh.load_mesh(ply_path, process=False)\n",
        "                            vertices = np.asarray(mesh.vertices, dtype=np.float32)\n",
        "                            if vertices.size > 0:\n",
        "                                break\n",
        "                        except Exception:\n",
        "                            continue\n",
        "\n",
        "            # Fallback to dummy vertices if loading fails\n",
        "            if vertices is None or vertices.size == 0:\n",
        "                print('Erorr never use dummy')\n",
        "                vertices = np.array([\n",
        "                    [-20, -20, -20], [20, -20, -20], [20, 20, -20], [-20, 20, -20],\n",
        "                    [-20, -20, 20], [20, -20, 20], [20, 20, 20], [-20, 20, 20]\n",
        "                ], dtype=np.float32)\n",
        "\n",
        "            # Sample vertices if too many\n",
        "            if vertices.shape[0] > config.NUM_POINTS:\n",
        "                indices = np.random.choice(vertices.shape[0], config.NUM_POINTS, replace=False)\n",
        "                vertices = vertices[indices]\n",
        "            self.object_models[obj_idx] = {\n",
        "                'name': obj_name,\n",
        "                'vertices_raw': vertices*0.001,  # meters\n",
        "                'vertices_gt' : None               # fill per-sample\n",
        "                }\n",
        "\n",
        "\n",
        "        print(f\"✓ Loaded models for {len(self.object_models)} objects\")\n",
        "\n",
        "    def get_depth_path(self, rgb_path):\n",
        "        \"\"\"Get corresponding depth image path\"\"\"\n",
        "        if not self.depth_dir:\n",
        "            return None\n",
        "        rgb_filename = os.path.basename(rgb_path)\n",
        "        depth_path = os.path.join(self.depth_dir, rgb_filename)\n",
        "        return depth_path if os.path.exists(depth_path) else None\n",
        "    def load_ground_truth(self, rgb_path):\n",
        "        \"\"\"Load ground truth pose and class from corresponding files\"\"\"\n",
        "        filename_no_ext = os.path.splitext(os.path.basename(rgb_path))[0]\n",
        "\n",
        "        # Default values\n",
        "        gt_class_id = 0\n",
        "\n",
        "\n",
        "        # Load YOLO label for bbox and class\n",
        "        yolo_label_path = rgb_path.replace('/images/', '/labels/').replace('.png', '.txt').replace('.jpg', '.txt')\n",
        "        if os.path.exists(yolo_label_path):\n",
        "            try:\n",
        "                flag_bbox=0\n",
        "                with open(yolo_label_path, 'r') as f:\n",
        "                  for lin in f.readlines():\n",
        "                    line = lin.strip().split()\n",
        "                    if len(line) == 5 and flag_bbox==0:\n",
        "                        yolo_class_id = int(line[0])\n",
        "                        gt_class_id = max(0, min(yolo_class_id - 1, len(self.data_config.get('names', [])) - 1))\n",
        "                        bbox_normalized = [float(x) for x in line[1:5]]\n",
        "                        flag_bbox=1\n",
        "                    elif len(line) == 4:\n",
        "                        gt_t = np.array([float(x) for x in line[1:]], dtype=np.float32)\n",
        "                    elif len(line) >= 7:\n",
        "                        gt_r = np.array([float(x) for x in line[1:]], dtype=np.float32)\n",
        "            except Exception:\n",
        "\n",
        "                pass\n",
        "        # convert t mm->m\n",
        "\n",
        "        # quaternion\n",
        "        # reorder to wxyz\n",
        "\n",
        "        rotation_matrix = np.array(gt_r).reshape((3, 3))\n",
        "        rot = R.from_matrix(rotation_matrix)\n",
        "        gt_t = gt_t / 1000.0\n",
        "        quat = rot.as_quat()  # xyzw\n",
        "        quat_wxyz = np.array([quat[3], quat[0], quat[1], quat[2]], np.float32)\n",
        "\n",
        "\n",
        "        return gt_class_id, gt_t, quat_wxyz, gt_r, bbox_normalized\n",
        "\n",
        "\n",
        "    def extract_patches_with_segmentation(self, rgb_image, depth_image, bbox_norm, mask=None):\n",
        "        \"\"\"Extract RGB and depth patches using segmentation mask\"\"\"\n",
        "        h, w = rgb_image.shape[:2]\n",
        "\n",
        "        # Convert normalized bbox to pixel coordinates\n",
        "        xc_n, yc_n, w_n, h_n = bbox_norm\n",
        "        xc_px = int(xc_n * w)\n",
        "        yc_px = int(yc_n * h)\n",
        "        w_px = int(w_n * w)\n",
        "        h_px = int(h_n * h)\n",
        "\n",
        "        x1 = max(0, xc_px - w_px // 2)\n",
        "        y1 = max(0, yc_px - h_px // 2)\n",
        "        x2 = min(w, xc_px + w_px // 2)\n",
        "        y2 = min(h, yc_px + h_px // 2)\n",
        "\n",
        "        # Ensure valid bounding box\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            x1, y1, x2, y2 = 0, 0, min(w, 100), min(h, 100)\n",
        "\n",
        "        # Extract RGB patch\n",
        "        rgb_patch = rgb_image[y1:y2, x1:x2].copy()\n",
        "\n",
        "        # Apply mask if available\n",
        "        if mask is not None:\n",
        "            mask_patch = mask[y1:y2, x1:x2]\n",
        "            if rgb_patch.shape[:2] == mask_patch.shape:\n",
        "                rgb_patch[mask_patch == 0] = 0\n",
        "\n",
        "        # Resize RGB patch\n",
        "        if rgb_patch.size == 0:\n",
        "            rgb_patch = np.zeros((self.patch_size, self.patch_size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            rgb_patch = cv2.resize(rgb_patch, (self.patch_size, self.patch_size))\n",
        "\n",
        "        # Extract depth patch\n",
        "        depth_patch = None\n",
        "        if depth_image is not None:\n",
        "            depth_patch = depth_image[y1:y2, x1:x2].copy()\n",
        "            if mask is not None:\n",
        "                mask_patch = mask[y1:y2, x1:x2]\n",
        "                if depth_patch.shape == mask_patch.shape:\n",
        "                    depth_patch[mask_patch == 0] = 0\n",
        "\n",
        "            if depth_patch.size > 0:\n",
        "                depth_patch = cv2.resize(depth_patch, (self.patch_size, self.patch_size),\n",
        "                                       interpolation=cv2.INTER_NEAREST)\n",
        "            else:\n",
        "                depth_patch = None\n",
        "\n",
        "        return rgb_patch, depth_patch, [x1, y1, x2, y2]\n",
        "\n",
        "    def depth_to_pointcloud(self, depth_patch, bbox_pixel):\n",
        "        \"\"\"Convert depth patch to point cloud\"\"\"\n",
        "        if depth_patch is None:\n",
        "            points = np.random.randn(self.num_points, 3).astype(np.float32) * 0.01\n",
        "            return points\n",
        "\n",
        "        h, w = depth_patch.shape\n",
        "        if h == 0 or w == 0:\n",
        "            points = np.random.randn(self.num_points, 3).astype(np.float32) * 0.01\n",
        "            return points\n",
        "\n",
        "        fx, fy = config.K[0, 0], config.K[1, 1]\n",
        "        cx, cy = config.K[0, 2], config.K[1, 2]\n",
        "\n",
        "        # Create coordinate grids\n",
        "        y_coords, x_coords = np.mgrid[0:h, 0:w]\n",
        "\n",
        "        # Map patch coordinates to original image coordinates\n",
        "        x1, y1, x2, y2 = bbox_pixel\n",
        "        scale_x = (x2 - x1) / w if w > 0 else 1\n",
        "        scale_y = (y2 - y1) / h if h > 0 else 1\n",
        "\n",
        "        x_coords_orig = x_coords * scale_x + x1\n",
        "        y_coords_orig = y_coords * scale_y + y1\n",
        "\n",
        "        # Flatten and filter valid depth values\n",
        "        x_flat = x_coords_orig.flatten()\n",
        "        y_flat = y_coords_orig.flatten()\n",
        "        z_flat = depth_patch.flatten()\n",
        "\n",
        "        valid_mask = (z_flat > 0) & (z_flat < 5.0)\n",
        "        x_valid = x_flat[valid_mask]\n",
        "        y_valid = y_flat[valid_mask]\n",
        "        z_valid = z_flat[valid_mask]\n",
        "\n",
        "        if len(z_valid) == 0:\n",
        "            points = np.random.randn(self.num_points, 3).astype(np.float32) * 0.01\n",
        "            return points\n",
        "\n",
        "        # Convert to 3D coordinates\n",
        "        points_x = (x_valid - cx) * z_valid / fx\n",
        "        points_y = (y_valid - cy) * z_valid / fy\n",
        "        points_z = z_valid\n",
        "\n",
        "        points_3d = np.column_stack((points_x, points_y, points_z))\n",
        "\n",
        "        # Sample to target number of points\n",
        "        if len(points_3d) > self.num_points:\n",
        "            indices = np.random.choice(len(points_3d), self.num_points, replace=False)\n",
        "            points_3d = points_3d[indices]\n",
        "        elif len(points_3d) < self.num_points:\n",
        "            if len(points_3d) == 0:\n",
        "                points_3d = np.random.randn(self.num_points, 3).astype(np.float32) * 0.01\n",
        "            else:\n",
        "                num_to_pad = self.num_points - len(points_3d)\n",
        "                pad_indices = np.random.choice(len(points_3d), num_to_pad, replace=True)\n",
        "                points_3d = np.vstack([points_3d, points_3d[pad_indices]])\n",
        "\n",
        "        return points_3d.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rgb_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a single dataset sample\"\"\"\n",
        "        rgb_path = self.rgb_paths[idx]\n",
        "\n",
        "        try:\n",
        "            # Load RGB image\n",
        "            rgb_image = cv2.imread(rgb_path)\n",
        "            if rgb_image is None:\n",
        "                raise FileNotFoundError(f\"Could not load RGB image: {rgb_path}\")\n",
        "            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Load depth image\n",
        "            depth_image = None\n",
        "            depth_path = self.get_depth_path(rgb_path)\n",
        "            if depth_path:\n",
        "                depth_image = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "                if depth_image is not None:\n",
        "                    depth_image = depth_image.astype(np.float32) / config.DEPTH_SCALE_MM_TO_M\n",
        "\n",
        "            # Load ground truth\n",
        "            gt_class_id, gt_t, gt_quat, gt_R, bbox_norm = self.load_ground_truth(rgb_path)\n",
        "            gt_pose_7d_list = [gt_t[0], gt_t[1], gt_t[2],gt_quat[0], gt_quat[1], gt_quat[2], gt_quat[3]]\n",
        "\n",
        "            # Convert the list to a NumPy array with the specified dtype\n",
        "            gt_pose_7d = np.array(gt_pose_7d_list, dtype=np.float32)\n",
        "            model = self.object_models[gt_class_id]\n",
        "            raw = model['vertices_raw']  # [M,3]\n",
        "            # transform raw -> GT camera\n",
        "\n",
        "            # Apply segmentation if enabled\n",
        "            object_mask = None\n",
        "            if self.use_segmentation and segmentation_module is not None:\n",
        "                try:\n",
        "                    bbox_pixel = convert_yolo_bbox_to_pixel(\n",
        "                        bbox_norm, rgb_image.shape[1], rgb_image.shape[0]\n",
        "                    )\n",
        "                    object_mask, _ = segmentation_module.refine_detection(\n",
        "                        rgb_image, bbox_pixel, gt_class_id\n",
        "                    )\n",
        "                except Exception:\n",
        "                    object_mask = None\n",
        "\n",
        "            # Extract patches\n",
        "            rgb_patch, depth_patch, bbox_pixel = self.extract_patches_with_segmentation(\n",
        "                rgb_image, depth_image, bbox_norm, object_mask\n",
        "            )\n",
        "\n",
        "            # Generate point cloud\n",
        "            points_3d = self.depth_to_pointcloud(depth_patch, bbox_pixel)\n",
        "\n",
        "            # Convert to tensors\n",
        "            rgb_tensor = torch.from_numpy(rgb_patch.transpose(2, 0, 1)).float() / 255.0\n",
        "            points_tensor = torch.from_numpy(points_3d).float()\n",
        "            gt_pose_tensor = torch.from_numpy(gt_pose_7d).float()\n",
        "            class_id_tensor = torch.tensor(gt_class_id, dtype=torch.long)\n",
        "\n",
        "            return {\n",
        "                'rgb': rgb_tensor,\n",
        "                'points': points_tensor,\n",
        "                'class_id': class_id_tensor,\n",
        "                'gt_pose': gt_pose_tensor,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return default tensors on error\n",
        "            print(f\"Error loading sample: {e}\")\n",
        "            print(f\"++++++++Error loading sample: please check and don't use default values \")\n",
        "            default_rgb = torch.zeros((3, self.patch_size, self.patch_size), dtype=torch.float32)\n",
        "            default_points = torch.zeros((self.num_points, 3), dtype=torch.float32)\n",
        "            default_class_id = torch.tensor(0, dtype=torch.long)\n",
        "            default_pose = torch.tensor([0.0, 0.0, 0.3, 1.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
        "\n",
        "            return {\n",
        "                'rgb': default_rgb,\n",
        "                'points': default_points,\n",
        "                'class_id': default_class_id,\n",
        "                'gt_pose': default_pose,\n",
        "            }\n",
        "\n",
        "def test_dataset():\n",
        "    \"\"\"Test the dataset implementation\"\"\"\n",
        "    try:\n",
        "        dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "        test_dataset = DenseFusionDataset(dataset_config, split='train', use_segmentation=False)\n",
        "\n",
        "        # Test loading a sample\n",
        "        sample = test_dataset[0]\n",
        "        print(f\"✓ Dataset test successful:\")\n",
        "        print(f\"  Dataset size: {len(test_dataset)}\")\n",
        "        print(f\"  RGB shape: {sample['rgb'].shape}\")\n",
        "        print(f\"  Points shape: {sample['points'].shape}\")\n",
        "        print(f\"  Class ID: {sample['class_id'].item()}\")\n",
        "        print(f\"  GT pose shape: {sample['gt_pose'].shape}\")\n",
        "\n",
        "        return test_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Dataset test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test the dataset\n",
        "test_dataset_obj = test_dataset()\n",
        "print(\"\\n✓ Block 8 completed: Dataset ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaFExHUd9qNv"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 9: TRAINING PIPELINE\n",
        "# ==============================================================================\n",
        "def load_checkpoint(path, model, optimizer=None, scheduler=None):\n",
        "    \"\"\"\n",
        "    Loads model—and optionally optimizer & scheduler—state from a checkpoint.\n",
        "    Returns the epoch to resume from.\n",
        "    \"\"\"\n",
        "\n",
        "    checkpoint = torch.load(path, map_location=config.DEVICE)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    print(f\"Resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n",
        "\n",
        "def create_data_loaders():\n",
        "    \"\"\"Create optimized data loaders\"\"\"\n",
        "    dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "\n",
        "    train_dataset = DenseFusionDataset(\n",
        "        dataset_config, split='train',\n",
        "        use_segmentation=config.USE_SEGMENTATION\n",
        "    )\n",
        "\n",
        "    val_dataset = DenseFusionDataset(\n",
        "        dataset_config, split='val',\n",
        "        use_segmentation=config.USE_SEGMENTATION\n",
        "    )\n",
        "\n",
        "    # Use num_workers=0 for Colab compatibility\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
        "        num_workers=0, pin_memory=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=0, pin_memory=False, drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_dataset\n",
        "\n",
        "def save_model_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, is_best=False):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'config': {\n",
        "            'num_points': config.NUM_POINTS,\n",
        "            'patch_size': config.PATCH_SIZE,\n",
        "            'batch_size': config.BATCH_SIZE,\n",
        "            'learning_rate': config.LEARNING_RATE\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create directories\n",
        "    create_directories()\n",
        "\n",
        "    if is_best:\n",
        "        # Save best model\n",
        "        best_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_full_info.pth' )\n",
        "        torch.save(checkpoint_data, best_path)\n",
        "\n",
        "        # Save simple state dict to drive\n",
        "        simple_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_densefusion_best.pth')\n",
        "        torch.save(model.state_dict(), simple_path)\n",
        "\n",
        "        print(f\"✓ Best model saved: {simple_path}\")\n",
        "        return simple_path\n",
        "    else:\n",
        "        # Save regular checkpoint\n",
        "        epoch_path = os.path.join(config.CHECKPOINTS_DIR, f'{config.MODELS_NAME}_checkpoint_epoch_{epoch:03d}.pth')\n",
        "        torch.save(checkpoint_data, epoch_path)\n",
        "        return epoch_path\n",
        "\n",
        "def create_training_plots(train_losses, val_losses, save_path=None):\n",
        "    \"\"\"Create and save training progress plots\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Loss curves\n",
        "        plt.subplot(1, 2, 1)\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "        plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "        plt.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "        plt.title('Training Progress')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Log scale\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.semilogy(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "        plt.semilogy(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "        plt.title('Training Progress (Log Scale)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (log)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create plots: {e}\")\n",
        "\n",
        "def train_densefusion():\n",
        "    \"\"\"Complete (simplified) DenseFusion training pipeline\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SIMPLIFIED DENSEFUSION TRAINING PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup\n",
        "    config.setup_environment()\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # Load data\n",
        "    train_loader, val_loader, train_dataset = create_data_loaders()\n",
        "    dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "    num_classes = len(dataset_config.get('names', []))\n",
        "\n",
        "    print(f\"Training setup:\")\n",
        "    print(f\"  Classes: {num_classes}\")\n",
        "    print(f\"  Train samples: {len(train_dataset)}\")\n",
        "    print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
        "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "    print(f\"  Device: {config.DEVICE}\")\n",
        "\n",
        "    # Initialize model and training components\n",
        "    #model = DenseFusionNetwork(num_objects=num_classes).to(config.DEVICE)\n",
        "    model = DenseFusionNetwork( num_objects=num_classes,\n",
        "                               use_transformer=config.USE_TRANSFORMER_FUSION\n",
        "                                ).to(config.DEVICE)\n",
        "    loss_fn = DenseFusionLoss(train_dataset.object_models, config.SYMMETRIC_LIST)\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    # Mixed precision scaler\n",
        "    scaler = None\n",
        "    if config.USE_MIXED_PRECISION and torch.cuda.is_available():\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Training state\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(f\"\\nStarting training...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        for epoch in range(config.NUM_EPOCHS):\n",
        "            print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss_accum = 0.0\n",
        "            train_batches = 0\n",
        "            loss_fn.reset_stats()\n",
        "\n",
        "            train_pbar = tqdm(train_loader, desc=f\"Training\", leave=False)\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_pbar):\n",
        "                try:\n",
        "\n",
        "                    # Move data to device\n",
        "                    rgb = batch['rgb'].to(config.DEVICE, non_blocking=True)\n",
        "                    points = batch['points'].to(config.DEVICE, non_blocking=True)\n",
        "                    gt_poses = batch['gt_pose'].to(config.DEVICE, non_blocking=True)\n",
        "                    class_ids = batch['class_id'].to(config.DEVICE, non_blocking=True)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    if scaler is not None:\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            pred_poses, pred_confs = model(rgb, points)\n",
        "                            total_loss, loss_dict = loss_fn(pred_poses,gt_poses, pred_confs, class_ids)\n",
        "                    else:\n",
        "                        pred_poses, pred_confs = model(rgb, points)\n",
        "                        total_loss, loss_dict = loss_fn(pred_poses,gt_poses, pred_confs, class_ids)\n",
        "\n",
        "                    # Backward pass\n",
        "                    if scaler is not None:\n",
        "                        scaler.scale(total_loss).backward()\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        total_loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                        optimizer.step()\n",
        "\n",
        "                    train_loss_accum += loss_dict['total_loss']\n",
        "                    train_batches += 1\n",
        "\n",
        "                    train_pbar.set_postfix({\n",
        "                        'Loss': f\"{loss_dict['total_loss']:.6f}\",\n",
        "                        'ADD': f\"{loss_dict['add_loss']:.6f}\"\n",
        "                    })\n",
        "\n",
        "                    # Memory cleanup\n",
        "                    if batch_idx % 10 == 0:\n",
        "                        cleanup_memory()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if \"out of memory\" in str(e):\n",
        "                        print(f\"\\nOOM at batch {batch_idx}, cleaning up...\")\n",
        "                        cleanup_memory()\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(f\"\\nError in batch {batch_idx}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            avg_train_loss = train_loss_accum / max(train_batches, 1)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss_accum = 0.0\n",
        "            val_batches = 0\n",
        "\n",
        "            val_pbar = tqdm(val_loader, desc=f\"Validation\", leave=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, batch in enumerate(val_pbar):\n",
        "                    try:\n",
        "                        rgb = batch['rgb'].to(config.DEVICE, non_blocking=True)\n",
        "                        points = batch['points'].to(config.DEVICE, non_blocking=True)\n",
        "                        gt_poses = batch['gt_pose'].to(config.DEVICE, non_blocking=True)\n",
        "                        class_ids = batch['class_id'].to(config.DEVICE, non_blocking=True)\n",
        "\n",
        "                        if scaler is not None:\n",
        "                            with torch.cuda.amp.autocast():\n",
        "                                pred_poses, pred_confs = model(rgb, points)\n",
        "                                total_loss, loss_dict = loss_fn(pred_poses,gt_poses, pred_confs, class_ids)\n",
        "                        else:\n",
        "                            pred_poses, pred_confs = model(rgb, points)\n",
        "                            total_loss, loss_dict = loss_fn(pred_poses,gt_poses, pred_confs, class_ids)\n",
        "\n",
        "                        val_loss_accum += loss_dict['total_loss']\n",
        "                        val_batches += 1\n",
        "\n",
        "                        val_pbar.set_postfix({'Val Loss': f\"{loss_dict['total_loss']:.4f}\"})\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "            avg_val_loss = val_loss_accum / max(val_batches, 1)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(avg_val_loss)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Epoch {epoch+1} Results:\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
        "            print(f\"  Val Loss: {avg_val_loss:.6f}\")\n",
        "            print(f\"  Learning Rate: {current_lr:.8f}\")\n",
        "\n",
        "            # Save best model\n",
        "            is_best = avg_val_loss < best_val_loss\n",
        "            if is_best:\n",
        "                best_val_loss = avg_val_loss\n",
        "                print(f\"  ✓ NEW BEST MODEL!\")\n",
        "\n",
        "            save_model_checkpoint(model, optimizer, scheduler, epoch,\n",
        "                                avg_train_loss, avg_val_loss, is_best)\n",
        "\n",
        "            # Create plots every 2 epochs\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                plot_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_training_progress.png')\n",
        "                create_training_plots(train_losses, val_losses, plot_path)\n",
        "\n",
        "            cleanup_memory()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTraining error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Save final results\n",
        "    total_time = (datetime.datetime.now() - start_time).total_seconds()\n",
        "\n",
        "    # Final model save\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_densefusion_final_{timestamp}.pth')\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'total_time': total_time,\n",
        "        'config_dict': config.__dict__\n",
        "    }, final_path)\n",
        "\n",
        "    # Final plots\n",
        "    if train_losses and val_losses:\n",
        "        final_plot_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_final_training_plot_{timestamp}.png')\n",
        "        create_training_plots(train_losses, val_losses, final_plot_path)\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING COMPLETED\")\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"Total time: {total_time:.1f} seconds\")\n",
        "    print(f\"Epochs completed: {len(train_losses)}\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "    if train_losses:\n",
        "        print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"Models saved to: {config.MODELS_SAVE_DIR}\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'total_time': total_time\n",
        "    }\n",
        "\n",
        "def load_trained_model(model_path=None,use_transformer=config.USE_TRANSFORMER_FUSION):\n",
        "    \"\"\"Load a trained DenseFusion model\"\"\"\n",
        "    if model_path is None:\n",
        "        # Try to find best model\n",
        "        model_path = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_densefusion_best.pth')\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model not found: {model_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load dataset config to get number of classes\n",
        "        dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "        num_classes = len(dataset_config.get('names', []))\n",
        "\n",
        "        # Create model\n",
        "        model = DenseFusionNetwork(num_objects=num_classes,use_transformer=use_transformer)\n",
        "\n",
        "        # Load weights\n",
        "        if model_path.endswith('best_model.pth'):\n",
        "            # Load from checkpoint\n",
        "            checkpoint = torch.load(model_path, map_location=config.DEVICE)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            # Load state dict directly\n",
        "            model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))\n",
        "\n",
        "        model = model.to(config.DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        model_type = \"Transformer\" if use_transformer else \"MLP\"\n",
        "        print(f\"✓ {model_type} model loaded from: {model_path}\")\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✓ Block 9 completed: Training pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn2cSR7d9n7D"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 10: EVALUATION PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "def detect_and_estimate_pose(yolo_model, pose_model, dataset, rgb_path, depth_path=None):\n",
        "    \"\"\"Complete detection and pose estimation pipeline\"\"\"\n",
        "    try:\n",
        "        # Load RGB image\n",
        "        rgb_image = cv2.imread(rgb_path)\n",
        "        if rgb_image is None:\n",
        "            return None\n",
        "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # YOLO detection\n",
        "        results = yolo_model(rgb_image, verbose=False)\n",
        "        if len(results) == 0 or len(results[0].boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        # Get best detection\n",
        "        result = results[0]\n",
        "        box = result.boxes[0]\n",
        "        conf = float(box.conf)\n",
        "        yolo_class_id = int(box.cls)\n",
        "        class_id = yolo_class_id - 1  # Convert to 0-based\n",
        "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "\n",
        "        # Ensure valid bbox\n",
        "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(rgb_image.shape[1], x2), min(rgb_image.shape[0], y2)\n",
        "\n",
        "        # Convert to normalized bbox\n",
        "        h, w = rgb_image.shape[:2]\n",
        "        xc_n = (x1 + x2) / (2 * w)\n",
        "        yc_n = (y1 + y2) / (2 * h)\n",
        "        w_n = (x2 - x1) / w\n",
        "        h_n = (y2 - y1) / h\n",
        "        bbox_normalized = [xc_n, yc_n, w_n, h_n]\n",
        "\n",
        "        # Apply segmentation if available\n",
        "        object_mask = None\n",
        "        if segmentation_module is not None:\n",
        "            try:\n",
        "                object_mask, _ = segmentation_module.refine_detection(\n",
        "                    rgb_image, [x1, y1, x2, y2], class_id\n",
        "                )\n",
        "            except Exception:\n",
        "                object_mask = None\n",
        "\n",
        "        # Load depth image\n",
        "        depth_image = None\n",
        "        if depth_path and os.path.exists(depth_path):\n",
        "            depth_image = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "            if depth_image is not None:\n",
        "                depth_image = depth_image.astype(np.float32) / config.DEPTH_SCALE_MM_TO_M\n",
        "\n",
        "        # Extract patches\n",
        "        rgb_patch, depth_patch, bbox_pixel = dataset.extract_patches_with_segmentation(\n",
        "            rgb_image, depth_image, bbox_normalized, object_mask\n",
        "        )\n",
        "\n",
        "        # Generate point cloud\n",
        "        points_3d = dataset.depth_to_pointcloud(depth_patch, bbox_pixel)\n",
        "\n",
        "        # Prepare for model inference\n",
        "        rgb_tensor = torch.from_numpy(rgb_patch.transpose(2, 0, 1)).float() / 255.0\n",
        "        rgb_tensor = rgb_tensor.unsqueeze(0).to(config.DEVICE)\n",
        "        points_tensor = torch.from_numpy(points_3d).unsqueeze(0).to(config.DEVICE)\n",
        "\n",
        "        # Pose estimation\n",
        "        with torch.no_grad():\n",
        "            pred_pose, pred_conf = pose_model(rgb_tensor, points_tensor)\n",
        "\n",
        "        pred_pose_np = pred_pose.cpu().numpy().flatten()\n",
        "        confidence = torch.sigmoid(pred_conf).cpu().numpy().item()\n",
        "\n",
        "        return {\n",
        "            'bbox': [x1, y1, x2, y2],\n",
        "            'class_id': class_id,\n",
        "            'pose': pred_pose_np,\n",
        "            'confidence': confidence,\n",
        "            'yolo_confidence': conf,\n",
        "            'mask': object_mask\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pose estimation: {e}\")\n",
        "        return None\n",
        "\n",
        "def evaluate_model_comprehensive(yolo_model, pose_model, test_dataset, model_diameters=None):\n",
        "    \"\"\"Comprehensive model evaluation with ADD metrics\"\"\"\n",
        "    print(f\"Starting comprehensive evaluation...\")\n",
        "    num_samples = min(config.MAX_EVAL_SAMPLES, len(test_dataset))\n",
        "    print(f\"Evaluating on {num_samples} samples\")\n",
        "\n",
        "    pose_model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    metrics = {\n",
        "        'add_values': [],\n",
        "        'add_2cm': [],\n",
        "        'add_5cm': [],\n",
        "        'add_10cm': [],\n",
        "        'yolo_confidence': [],\n",
        "        'pose_confidence': [],\n",
        "        'class_ids': [],\n",
        "        'success_by_class': {},\n",
        "        'detection_rate': 0,\n",
        "        'pose_estimates': []\n",
        "    }\n",
        "\n",
        "    # Add diameter-based metrics if available\n",
        "    if model_diameters is not None:\n",
        "        metrics.update({'add_5p': [], 'add_10p': [], 'add_20p': []})\n",
        "\n",
        "    # Evaluation loop\n",
        "    for i in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
        "        try:\n",
        "            # Load sample\n",
        "            sample = test_dataset[i]\n",
        "            rgb_path = test_dataset.rgb_paths[i]\n",
        "            gt_pose = sample['gt_pose'].cpu().numpy()\n",
        "            class_id = sample['class_id'].item()\n",
        "            depth_path = test_dataset.get_depth_path(rgb_path)\n",
        "\n",
        "            # Run detection and pose estimation\n",
        "            result = detect_and_estimate_pose(\n",
        "                yolo_model, pose_model, test_dataset, rgb_path, depth_path\n",
        "            )\n",
        "\n",
        "            if result is None:\n",
        "                # No detection - record as failure\n",
        "                metrics['add_values'].append(float('inf'))\n",
        "                metrics['add_2cm'].append(0)\n",
        "                metrics['add_5cm'].append(0)\n",
        "                metrics['add_10cm'].append(0)\n",
        "                metrics['yolo_confidence'].append(0)\n",
        "                metrics['pose_confidence'].append(0)\n",
        "                metrics['class_ids'].append(class_id)\n",
        "\n",
        "                if model_diameters is not None:\n",
        "                    metrics['add_5p'].append(0)\n",
        "                    metrics['add_10p'].append(0)\n",
        "                    metrics['add_20p'].append(0)\n",
        "                continue\n",
        "\n",
        "            # Extract prediction results\n",
        "            pred_pose = result['pose']\n",
        "            yolo_conf = result['yolo_confidence']\n",
        "            pose_conf = result['confidence']\n",
        "\n",
        "            # Compute ADD metrics if valid model available\n",
        "            if class_id in test_dataset.object_models:\n",
        "                model_vertices = test_dataset.object_models[class_id]['vertices_raw']\n",
        "\n",
        "                if model_vertices.shape[0] > 10:\n",
        "                    diameter = model_diameters.get(class_id, None) if model_diameters else None\n",
        "\n",
        "                    # Compute ADD metrics\n",
        "                    add_metrics = compute_add_metrics_with_thresholds(\n",
        "                        pred_pose, gt_pose, class_id, config.SYMMETRIC_LIST, model_vertices, diameter\n",
        "                    )\n",
        "\n",
        "                    metrics['add_values'].append(add_metrics['add_value'])\n",
        "                    metrics['add_2cm'].append(int(add_metrics['add_success_2cm']))\n",
        "                    metrics['add_5cm'].append(int(add_metrics['add_success_5cm']))\n",
        "                    metrics['add_10cm'].append(int(add_metrics['add_success_10cm']))\n",
        "\n",
        "                    if model_diameters is not None and diameter is not None:\n",
        "                        metrics['add_5p'].append(int(add_metrics['add_success_5p']))\n",
        "                        metrics['add_10p'].append(int(add_metrics['add_success_10p']))\n",
        "                        metrics['add_20p'].append(int(add_metrics['add_success_20p']))\n",
        "                    elif model_diameters is not None:\n",
        "                        metrics['add_5p'].append(0)\n",
        "                        metrics['add_10p'].append(0)\n",
        "                        metrics['add_20p'].append(0)\n",
        "                else:\n",
        "                    # Invalid model - record as failure\n",
        "                    metrics['add_values'].append(float('inf'))\n",
        "                    metrics['add_2cm'].append(0)\n",
        "                    metrics['add_5cm'].append(0)\n",
        "                    metrics['add_10cm'].append(0)\n",
        "                    if model_diameters is not None:\n",
        "                        metrics['add_5p'].append(0)\n",
        "                        metrics['add_10p'].append(0)\n",
        "                        metrics['add_20p'].append(0)\n",
        "            else:\n",
        "                # No model available - record as failure\n",
        "                metrics['add_values'].append(float('inf'))\n",
        "                metrics['add_2cm'].append(0)\n",
        "                metrics['add_5cm'].append(0)\n",
        "                metrics['add_10cm'].append(0)\n",
        "                if model_diameters is not None:\n",
        "                    metrics['add_5p'].append(0)\n",
        "                    metrics['add_10p'].append(0)\n",
        "                    metrics['add_20p'].append(0)\n",
        "\n",
        "            # Record confidence scores and other metrics\n",
        "            metrics['yolo_confidence'].append(yolo_conf)\n",
        "            metrics['pose_confidence'].append(pose_conf)\n",
        "            metrics['class_ids'].append(class_id)\n",
        "\n",
        "            # Per-class tracking\n",
        "            if class_id not in metrics['success_by_class']:\n",
        "                metrics['success_by_class'][class_id] = {\n",
        "                    'count': 0, 'success_2cm': 0, 'success_5cm': 0, 'success_10cm': 0\n",
        "                }\n",
        "                if model_diameters is not None:\n",
        "                    metrics['success_by_class'][class_id].update({\n",
        "                        'success_5p': 0, 'success_10p': 0, 'success_20p': 0\n",
        "                    })\n",
        "\n",
        "            metrics['success_by_class'][class_id]['count'] += 1\n",
        "            metrics['success_by_class'][class_id]['success_2cm'] += metrics['add_2cm'][-1]\n",
        "            metrics['success_by_class'][class_id]['success_5cm'] += metrics['add_5cm'][-1]\n",
        "            metrics['success_by_class'][class_id]['success_10cm'] += metrics['add_10cm'][-1]\n",
        "\n",
        "            if model_diameters is not None:\n",
        "                metrics['success_by_class'][class_id]['success_5p'] += metrics['add_5p'][-1]\n",
        "                metrics['success_by_class'][class_id]['success_10p'] += metrics['add_10p'][-1]\n",
        "                metrics['success_by_class'][class_id]['success_20p'] += metrics['add_20p'][-1]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    detection_count = sum(1 for v in metrics['add_values'] if v < float('inf'))\n",
        "    total_samples = len(metrics['add_values'])\n",
        "\n",
        "    metrics['detection_rate'] = detection_count / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    # Calculate mean ADD only from valid detections\n",
        "    valid_add_values = [v for v in metrics['add_values'] if v < float('inf')]\n",
        "    metrics['mean_add'] = np.mean(valid_add_values) if valid_add_values else float('inf')\n",
        "\n",
        "    metrics['success_rate_2cm'] = np.mean(metrics['add_2cm']) if metrics['add_2cm'] else 0\n",
        "    metrics['success_rate_5cm'] = np.mean(metrics['add_5cm']) if metrics['add_5cm'] else 0\n",
        "    metrics['success_rate_10cm'] = np.mean(metrics['add_10cm']) if metrics['add_10cm'] else 0\n",
        "\n",
        "    if model_diameters is not None:\n",
        "        metrics['success_rate_5p'] = np.mean(metrics['add_5p']) if metrics['add_5p'] else 0\n",
        "        metrics['success_rate_10p'] = np.mean(metrics['add_10p']) if metrics['add_10p'] else 0\n",
        "        metrics['success_rate_20p'] = np.mean(metrics['add_20p']) if metrics['add_20p'] else 0\n",
        "\n",
        "    # Calculate per-class success rates\n",
        "    for class_id, stats in metrics['success_by_class'].items():\n",
        "        if stats['count'] > 0:\n",
        "            for key in ['success_2cm', 'success_5cm', 'success_10cm']:\n",
        "                stats[f'rate_{key}'] = stats[key] / stats['count']\n",
        "            if model_diameters is not None:\n",
        "                for key in ['success_5p', 'success_10p', 'success_20p']:\n",
        "                    stats[f'rate_{key}'] = stats[key] / stats['count']\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Overall Performance:\")\n",
        "    print(f\"  Samples evaluated: {total_samples}\")\n",
        "    print(f\"  Detection rate: {metrics['detection_rate']:.4f} ({detection_count}/{total_samples})\")\n",
        "    print(f\"  Mean ADD error: {metrics['mean_add']:.4f} m\")\n",
        "    print(f\"  Success rate (<2cm): {metrics['success_rate_2cm']:.4f}\")\n",
        "    print(f\"  Success rate (<5cm): {metrics['success_rate_5cm']:.4f}\")\n",
        "    print(f\"  Success rate (<10cm): {metrics['success_rate_10cm']:.4f}\")\n",
        "\n",
        "    if model_diameters is not None:\n",
        "        print(f\"  Success rate (<5% diameter): {metrics['success_rate_5p']:.4f}\")\n",
        "        print(f\"  Success rate (<10% diameter): {metrics['success_rate_10p']:.4f}\")\n",
        "        print(f\"  Success rate (<20% diameter): {metrics['success_rate_20p']:.4f}\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    return metrics\n",
        "\n",
        "def run_complete_evaluation():\n",
        "    \"\"\"Run the complete evaluation pipeline\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPLETE SIMPLIFIED DENSEFUSION EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Load models and data\n",
        "        yolo_model = load_yolo_model(config.YOLO_MODEL_PATH)\n",
        "        if yolo_model is None:\n",
        "            return None\n",
        "\n",
        "        dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "        model_diameters = load_model_diameters(config.DIAMETER_INFO_PATH)\n",
        "\n",
        "        # Create test dataset\n",
        "        test_dataset = DenseFusionDataset(\n",
        "            dataset_config, split='val',\n",
        "            use_segmentation=config.USE_SEGMENTATION\n",
        "        )\n",
        "\n",
        "        # Load DenseFusion model\n",
        "        pose_model = load_trained_model(use_transformer=config.USE_TRANSFORMER_FUSION)\n",
        "        if pose_model is None:\n",
        "            print(\"⚠ No trained model found. Using random initialization.\")\n",
        "            num_classes = len(dataset_config.get('names', []))\n",
        "            pose_model = DenseFusionNetwork(num_objects=num_classes,use_transformer=config.USE_TRANSFORMER_FUSION).to(config.DEVICE)\n",
        "\n",
        "        # Run evaluation\n",
        "        metrics = evaluate_model_comprehensive(\n",
        "            yolo_model, pose_model, test_dataset, model_diameters\n",
        "        )\n",
        "\n",
        "        # Save evaluation results\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = os.path.join(config.MODELS_SAVE_DIR, f'{config.MODELS_NAME}_evaluation_results_{timestamp}.json')\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        json_metrics = {}\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                json_metrics[key] = value.tolist()\n",
        "            elif isinstance(value, dict):\n",
        "                json_metrics[key] = {}\n",
        "                for k, v in value.items():\n",
        "                    if isinstance(v, np.ndarray):\n",
        "                        json_metrics[key][k] = v.tolist()\n",
        "                    elif isinstance(v, (np.integer, np.floating)):\n",
        "                        json_metrics[key][k] = float(v)\n",
        "                    else:\n",
        "                        json_metrics[key][k] = v\n",
        "            elif isinstance(value, (np.integer, np.floating)):\n",
        "                json_metrics[key] = float(value)\n",
        "            else:\n",
        "                json_metrics[key] = value\n",
        "\n",
        "        try:\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(json_metrics, f, indent=2)\n",
        "            print(f\"✓ Evaluation results saved to: {results_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to save results: {e}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Evaluation failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"✓ Block 10 completed: Evaluation pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwoYPX579n9U"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 11: VISUALIZATION BLOCK - WIP DOES IT WORK??\n",
        "# ==============================================================================\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "\n",
        "def get_all_objects_in_image(dataset, sample_idx):\n",
        "    \"\"\"Find ALL objects in the same image by reading the YOLO label file\"\"\"\n",
        "    try:\n",
        "        # Check if dataset is valid\n",
        "        if not hasattr(dataset, 'rgb_paths') or sample_idx >= len(dataset.rgb_paths):\n",
        "            print(f\"⚠️ Invalid dataset or sample_idx {sample_idx}\")\n",
        "            return []\n",
        "\n",
        "        rgb_path = dataset.rgb_paths[sample_idx]\n",
        "        label_path = rgb_path.replace('/images/', '/labels/').replace('.png', '.txt').replace('.jpg', '.txt')\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            return [{'sample_idx': sample_idx, 'class_id': 0, 'bbox_norm': [0.5, 0.5, 0.3, 0.3],\n",
        "                    'rgb_path': rgb_path, 'object_idx': 0}]\n",
        "\n",
        "        objects = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line_idx, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) == 5:\n",
        "                        yolo_class_id = int(parts[0])\n",
        "                        class_id = yolo_class_id - 1\n",
        "                        bbox_norm = [float(x) for x in parts[1:5]]\n",
        "\n",
        "                    elif len(parts) == 4:\n",
        "\n",
        "                        gt_t = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
        "                    elif len(parts) >= 7:\n",
        "\n",
        "\n",
        "                        gt_r = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
        "\n",
        "            rotation_matrix = np.array(gt_r).reshape((3, 3))\n",
        "            rot = R.from_matrix(rotation_matrix)\n",
        "            gt_t = gt_t / 1000.0\n",
        "            # quaternion\n",
        "            quat = rot.as_quat()  # xyzw\n",
        "            # reorder to wxyz\n",
        "            quat_wxyz = np.array([quat[3], quat[0], quat[1], quat[2]], np.float32)\n",
        "            gt_pose_7d_list = [gt_t[0], gt_t[1], gt_t[2],quat_wxyz[0], quat_wxyz[1], quat_wxyz[2], quat_wxyz[3]]\n",
        "            gt_pose_7d = np.array(gt_pose_7d_list, dtype=np.float32)\n",
        "            gt_pose_tensor = torch.from_numpy(gt_pose_7d).float()\n",
        "\n",
        "            objects.append({\n",
        "                'sample_idx': sample_idx,\n",
        "                'class_id': class_id,\n",
        "                'bbox_norm': bbox_norm,\n",
        "                'rgb_path': rgb_path,\n",
        "                'object_idx': line_idx,\n",
        "                'pose':gt_pose_tensor\n",
        "            })\n",
        "\n",
        "        return objects if objects else [{'sample_idx': sample_idx, 'class_id': 0,\n",
        "                                       'bbox_norm': [0.5, 0.5, 0.3, 0.3], 'rgb_path': rgb_path, 'object_idx': 0}]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_all_objects_in_image: {e}\")\n",
        "        return []\n",
        "\n",
        "def create_sample_for_specific_object(dataset, object_info, rgb_image=None):\n",
        "    \"\"\"Create a sample dict for a specific object\"\"\"\n",
        "    try:\n",
        "        sample_idx = object_info['sample_idx']\n",
        "        class_id = object_info['class_id']\n",
        "        bbox_norm = object_info['bbox_norm']\n",
        "        # Load RGB image if not provided\n",
        "        if rgb_image is None:\n",
        "            rgb_path = object_info['rgb_path']\n",
        "            rgb_image = cv2.imread(rgb_path)\n",
        "            if rgb_image is not None:\n",
        "                rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if rgb_image is None:\n",
        "            return None\n",
        "\n",
        "        # Load depth image\n",
        "        depth_image = None\n",
        "        depth_path = dataset.get_depth_path(object_info['rgb_path'])\n",
        "        if depth_path and os.path.exists(depth_path):\n",
        "            depth_image = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "            if depth_image is not None:\n",
        "                depth_image = depth_image.astype(np.float32) / config.DEPTH_SCALE_MM_TO_M\n",
        "\n",
        "        # Extract patches for this specific object\n",
        "        rgb_patch, depth_patch, bbox_pixel = dataset.extract_patches_with_segmentation(\n",
        "            rgb_image, depth_image, bbox_norm, mask=None\n",
        "        )\n",
        "\n",
        "        # Generate point cloud\n",
        "        points_3d = dataset.depth_to_pointcloud(depth_patch, bbox_pixel)\n",
        "\n",
        "        # Create tensors\n",
        "        rgb_tensor = torch.from_numpy(rgb_patch.transpose(2, 0, 1)).float() / 255.0\n",
        "        points_tensor = torch.from_numpy(points_3d).float()\n",
        "\n",
        "        gt_pose_tensor = object_info['pose']\n",
        "\n",
        "        return {\n",
        "            'rgb': rgb_tensor,\n",
        "            'points': points_tensor,\n",
        "            'class_id': torch.tensor(class_id, dtype=torch.long),\n",
        "            'gt_pose': gt_pose_tensor,\n",
        "            'object_info': object_info\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def compute_add_visualization_fixed(pred_pose, gt_pose, vertices):\n",
        "    \"\"\" ADD computation for visualization\"\"\"\n",
        "    try:\n",
        "        # Convert to numpy\n",
        "        if torch.is_tensor(pred_pose):\n",
        "            pred_pose = pred_pose.detach().cpu().numpy()\n",
        "        if torch.is_tensor(gt_pose):\n",
        "            gt_pose = gt_pose.detach().cpu().numpy()\n",
        "        if torch.is_tensor(vertices):\n",
        "            vertices = vertices.detach().cpu().numpy()\n",
        "\n",
        "        # Normalize quaternions\n",
        "        pred_quat_norm = np.linalg.norm(pred_pose[3:])\n",
        "        gt_quat_norm = np.linalg.norm(gt_pose[3:])\n",
        "\n",
        "        if pred_quat_norm > 0:\n",
        "            pred_pose[3:] = pred_pose[3:] / pred_quat_norm\n",
        "        if gt_quat_norm > 0:\n",
        "            gt_pose[3:] = gt_pose[3:] / gt_quat_norm\n",
        "\n",
        "        # Convert vertices to meters if in mm\n",
        "        if np.max(np.abs(vertices)) > 1.0:\n",
        "            print('conveting_vertices')\n",
        "            vertices_m = vertices * 0.001\n",
        "        else:\n",
        "            vertices_m = vertices\n",
        "\n",
        "        # Decompose poses - DenseFusion format: [tx, ty, tz, qw, qx, qy, qz]\n",
        "        gt_t = gt_pose[:3]\n",
        "        gt_quat_wxyz = gt_pose[3:]\n",
        "        gt_quat_xyzw = [gt_quat_wxyz[1], gt_quat_wxyz[2], gt_quat_wxyz[3], gt_quat_wxyz[0]]\n",
        "        gt_R = R.from_quat(gt_quat_xyzw).as_matrix()\n",
        "\n",
        "        pred_t = pred_pose[:3]\n",
        "        pred_quat_wxyz = pred_pose[3:]\n",
        "        pred_quat_xyzw = [pred_quat_wxyz[1], pred_quat_wxyz[2], pred_quat_wxyz[3], pred_quat_wxyz[0]]\n",
        "        pred_R = R.from_quat(pred_quat_xyzw).as_matrix()\n",
        "\n",
        "        # Transform vertices\n",
        "        gt_points = (gt_R @ vertices_m.T).T + gt_t\n",
        "        pred_points = (pred_R @ vertices_m.T).T + pred_t\n",
        "\n",
        "        # Compute ADD\n",
        "        distances = np.linalg.norm(gt_points - pred_points, axis=1)\n",
        "        add_error = np.mean(distances)\n",
        "\n",
        "        return add_error, pred_points, gt_points\n",
        "\n",
        "    except Exception as e:\n",
        "        return float('inf'), None, None\n",
        "\n",
        "def compute_add_s_visualization_fixed(pred_pose, gt_pose, vertices):\n",
        "    \"\"\"ADD-S computation for visualization (symmetric objects)\"\"\"\n",
        "    try:\n",
        "        # Convert to numpy\n",
        "        if torch.is_tensor(pred_pose):\n",
        "            pred_pose = pred_pose.detach().cpu().numpy()\n",
        "        if torch.is_tensor(gt_pose):\n",
        "            gt_pose = gt_pose.detach().cpu().numpy()\n",
        "        if torch.is_tensor(vertices):\n",
        "            vertices = vertices.detach().cpu().numpy()\n",
        "\n",
        "        # Normalize quaternions\n",
        "        pred_quat_norm = np.linalg.norm(pred_pose[3:])\n",
        "        gt_quat_norm = np.linalg.norm(gt_pose[3:])\n",
        "        if pred_quat_norm > 0:\n",
        "            pred_pose[3:] = pred_pose[3:] / pred_quat_norm\n",
        "        if gt_quat_norm > 0:\n",
        "            gt_pose[3:] = gt_pose[3:] / gt_quat_norm\n",
        "\n",
        "        # Convert vertices to meters if in mm\n",
        "        if np.max(np.abs(vertices)) > 1.0:\n",
        "            print('converting_vertices')\n",
        "            vertices_m = vertices * 0.001\n",
        "        else:\n",
        "            vertices_m = vertices\n",
        "\n",
        "        # Decompose poses\n",
        "        gt_t = gt_pose[:3]\n",
        "        gt_quat_wxyz = gt_pose[3:]\n",
        "        gt_quat_xyzw = [gt_quat_wxyz[1], gt_quat_wxyz[2], gt_quat_wxyz[3], gt_quat_wxyz[0]]\n",
        "        gt_R = R.from_quat(gt_quat_xyzw).as_matrix()\n",
        "\n",
        "        pred_t = pred_pose[:3]\n",
        "        pred_quat_wxyz = pred_pose[3:]\n",
        "        pred_quat_xyzw = [pred_quat_wxyz[1], pred_quat_wxyz[2], pred_quat_wxyz[3], pred_quat_wxyz[0]]\n",
        "        pred_R = R.from_quat(pred_quat_xyzw).as_matrix()\n",
        "\n",
        "        # Transform vertices\n",
        "        gt_points = (gt_R @ vertices_m.T).T + gt_t\n",
        "        pred_points = (pred_R @ vertices_m.T).T + pred_t\n",
        "\n",
        "        # Compute ADD-S (nearest-neighbor distance)\n",
        "        tree = cKDTree(gt_points)\n",
        "        distances, _ = tree.query(pred_points, k=1)\n",
        "        add_s_error = np.mean(distances)\n",
        "\n",
        "        return add_s_error, pred_points, gt_points\n",
        "\n",
        "    except Exception as e:\n",
        "        return float('inf'), None, None\n",
        "\n",
        "def draw_bboxes_on_image(rgb_image, objects_info, predictions):\n",
        "    \"\"\"Draw bounding boxes and labels on the original image\"\"\"\n",
        "    image_with_boxes = rgb_image.copy()\n",
        "    h, w = image_with_boxes.shape[:2]\n",
        "\n",
        "    colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
        "\n",
        "    for i, (obj_info, pred_info) in enumerate(zip(objects_info, predictions)):\n",
        "        color = colors[i % len(colors)]\n",
        "\n",
        "        # Convert normalized bbox to pixel coordinates\n",
        "        bbox_norm = obj_info['bbox_norm']\n",
        "        xc_n, yc_n, w_n, h_n = bbox_norm\n",
        "\n",
        "        xc_px = int(xc_n * w)\n",
        "        yc_px = int(yc_n * h)\n",
        "        w_px = int(w_n * w)\n",
        "        h_px = int(h_n * h)\n",
        "\n",
        "        x1 = max(0, xc_px - w_px // 2)\n",
        "        y1 = max(0, yc_px - h_px // 2)\n",
        "        x2 = min(w, xc_px + w_px // 2)\n",
        "        y2 = min(h, yc_px + h_px // 2)\n",
        "\n",
        "        # Draw rectangle\n",
        "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), color, 3)\n",
        "\n",
        "        # Prepare labels\n",
        "        class_id = obj_info['class_id']\n",
        "        object_name = pred_info.get('object_name', f'Class_{class_id}')\n",
        "        add_error = pred_info.get('add_error', 0)\n",
        "        rot_errors = pred_info.get('rot_errors', {})\n",
        "\n",
        "        label = f\"{object_name}\"\n",
        "        metric_text1 = f\"ADD: {add_error:.3f}m\"\n",
        "        metric_text2 = f\"Rot: {rot_errors.get('overall', 0):.1f}°\"\n",
        "        metric_text3 = f\"X:{rot_errors.get('x_axis', 0):.1f}° Y:{rot_errors.get('y_axis', 0):.1f}° Z:{rot_errors.get('z_axis', 0):.1f}°\"\n",
        "\n",
        "        # Draw text background and labels\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_scale = 0.5\n",
        "        thickness = 1\n",
        "\n",
        "        # Calculate text sizes and draw background\n",
        "        (w1, h1), _ = cv2.getTextSize(label, font, font_scale + 0.1, thickness + 1)\n",
        "        (w2, h2), _ = cv2.getTextSize(metric_text1, font, font_scale, thickness)\n",
        "        (w3, h3), _ = cv2.getTextSize(metric_text2, font, font_scale, thickness)\n",
        "        (w4, h4), _ = cv2.getTextSize(metric_text3, font, font_scale - 0.1, thickness)\n",
        "\n",
        "        max_width = max(w1, w2, w3, w4) + 10\n",
        "        total_height = h1 + h2 + h3 + h4 + 20\n",
        "\n",
        "        cv2.rectangle(image_with_boxes, (x1, y1 - total_height), (x1 + max_width, y1), color, -1)\n",
        "\n",
        "        # Draw text lines\n",
        "        text_y = y1 - total_height + h1 + 5\n",
        "        cv2.putText(image_with_boxes, label, (x1 + 5, text_y), font, font_scale + 0.1, (255, 255, 255), thickness + 1)\n",
        "\n",
        "        text_y += h2 + 3\n",
        "        cv2.putText(image_with_boxes, metric_text1, (x1 + 5, text_y), font, font_scale, (255, 255, 255), thickness)\n",
        "\n",
        "        text_y += h3 + 3\n",
        "        cv2.putText(image_with_boxes, metric_text2, (x1 + 5, text_y), font, font_scale, (255, 255, 255), thickness)\n",
        "\n",
        "        text_y += h4 + 3\n",
        "        cv2.putText(image_with_boxes, metric_text3, (x1 + 5, text_y), font, font_scale - 0.1, (255, 255, 255), thickness)\n",
        "\n",
        "    return image_with_boxes\n",
        "\n",
        "def visualize_enhanced_all_objects(pose_model, dataset, sample_idx=0):\n",
        "    \"\"\"Enhanced visualization with rotation differences and original image\"\"\"\n",
        "    print(f\"🎯 Enhanced Visualization for sample {sample_idx}\")\n",
        "\n",
        "    # Validate inputs\n",
        "    if pose_model is None:\n",
        "        print(\"❌ Pose model is None - model loading failed\")\n",
        "        return None\n",
        "\n",
        "    if not hasattr(dataset, 'rgb_paths'):\n",
        "        print(\"❌ Invalid dataset - missing rgb_paths attribute\")\n",
        "        return None\n",
        "\n",
        "    if sample_idx >= len(dataset.rgb_paths):\n",
        "        print(f\"❌ Sample index {sample_idx} out of range. Dataset has {len(dataset.rgb_paths)} samples\")\n",
        "        return None\n",
        "\n",
        "    # Find all objects in the image\n",
        "    objects_in_image = get_all_objects_in_image(dataset, sample_idx)\n",
        "    if len(objects_in_image) == 0:\n",
        "        print(\"❌ No objects found in image\")\n",
        "        return None\n",
        "\n",
        "    # Load RGB image\n",
        "    rgb_path = dataset.rgb_paths[sample_idx]\n",
        "    rgb_image = cv2.imread(rgb_path)\n",
        "    if rgb_image is not None:\n",
        "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        print(f\"❌ Could not load RGB image: {rgb_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📷 Image: {os.path.basename(rgb_path)}\")\n",
        "    print(f\"📦 Objects found: {len(objects_in_image)}\")\n",
        "\n",
        "    # Process each object\n",
        "    pose_model.eval()\n",
        "    predictions = []\n",
        "    total_add = 0\n",
        "    total_rot = 0\n",
        "    valid_objects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for obj_idx, object_info in enumerate(objects_in_image):\n",
        "            sample = create_sample_for_specific_object(dataset, object_info, rgb_image)\n",
        "            if sample is None:\n",
        "                continue\n",
        "\n",
        "            class_id = object_info['class_id']\n",
        "            object_names = dataset.data_config.get('names', [])\n",
        "            object_name = object_names[class_id] if class_id < len(object_names) else f\"Class_{class_id}\"\n",
        "\n",
        "            # Get prediction\n",
        "            rgb_batch = sample['rgb'].unsqueeze(0).to(config.DEVICE)\n",
        "            points_batch = sample['points'].unsqueeze(0).to(config.DEVICE)\n",
        "\n",
        "            try:\n",
        "                pred_poses, pred_confs = pose_model(rgb_batch, points_batch)\n",
        "                pred_pose = pred_poses[0].cpu().numpy()\n",
        "                gt_pose = sample['gt_pose'].cpu().numpy()\n",
        "\n",
        "                # Get model vertices\n",
        "                if class_id not in dataset.object_models:\n",
        "                    continue\n",
        "\n",
        "                vertices = dataset.object_models[class_id]['vertices_raw']\n",
        "\n",
        "                # Compute ADD and rotation differences\n",
        "                if class_id in config.SYMMETRIC_LIST:\n",
        "                    add_error, pred_points, gt_points = compute_add_s_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "                else:\n",
        "                    add_error, pred_points, gt_points = compute_add_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "\n",
        "                rot_errors = compute_rotation_difference_degrees(pred_pose, gt_pose)\n",
        "\n",
        "                if pred_points is None:\n",
        "                    continue\n",
        "\n",
        "                predictions.append({\n",
        "                    'object_name': object_name,\n",
        "                    'class_id': class_id,\n",
        "                    'add_error': add_error,\n",
        "                    'rot_errors': rot_errors,\n",
        "                    'pred_points': pred_points,\n",
        "                    'gt_points': gt_points,\n",
        "                    'pred_pose': pred_pose,\n",
        "                    'gt_pose': gt_pose\n",
        "                })\n",
        "\n",
        "                total_add += add_error\n",
        "                total_rot += rot_errors['overall']\n",
        "                valid_objects += 1\n",
        "\n",
        "                print(f\"   ✅ {object_name}: ADD={add_error:.4f}m, Rot={rot_errors['overall']:.1f}°\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Prediction failed for {object_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if valid_objects == 0:\n",
        "        print(\"❌ No valid predictions to visualize\")\n",
        "        return None\n",
        "\n",
        "    # Create visualization\n",
        "    print(f\"\\n🎨 Creating enhanced visualization...\")\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n_objects = len(predictions)\n",
        "    if n_objects == 1:\n",
        "        rows, cols = 1, 2\n",
        "    elif n_objects == 2:\n",
        "        rows, cols = 2, 2\n",
        "    elif n_objects <= 4:\n",
        "        rows, cols = 2, 3\n",
        "    else:\n",
        "        rows = int(np.ceil((n_objects + 1) / 3))\n",
        "        cols = 3\n",
        "\n",
        "    # Create subplot specifications\n",
        "    subplot_specs = []\n",
        "    subplot_titles = []\n",
        "\n",
        "    # First subplot for original image\n",
        "    subplot_specs.append([{\"type\": \"xy\"}])\n",
        "    subplot_titles.append(\"Original Image with Detections\")\n",
        "\n",
        "    # Add 3D subplots for each object\n",
        "    for pred in predictions:\n",
        "        subplot_specs.append([{\"type\": \"scene\"}])\n",
        "        subplot_titles.append(f\"{pred['object_name']}\")\n",
        "\n",
        "    # Adjust specs for layout\n",
        "    if len(subplot_specs) <= 3:\n",
        "        spec_list = [[spec[0] for spec in subplot_specs]]\n",
        "    else:\n",
        "        spec_list = []\n",
        "        for i in range(0, len(subplot_specs), cols):\n",
        "            row_specs = []\n",
        "            for j in range(cols):\n",
        "                if i + j < len(subplot_specs):\n",
        "                    row_specs.append(subplot_specs[i + j][0])\n",
        "                else:\n",
        "                    row_specs.append({\"type\": \"xy\"})\n",
        "            spec_list.append(row_specs)\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(\n",
        "        rows=len(spec_list),\n",
        "        cols=cols,\n",
        "        subplot_titles=subplot_titles[:len(predictions) + 1],\n",
        "        specs=spec_list,\n",
        "        horizontal_spacing=0.05,\n",
        "        vertical_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # Add original image with bounding boxes\n",
        "    image_with_boxes = draw_bboxes_on_image(rgb_image, objects_in_image, predictions)\n",
        "    fig.add_trace(go.Image(z=image_with_boxes), row=1, col=1)\n",
        "\n",
        "    # Add 3D plots for each object\n",
        "    for i, pred in enumerate(predictions):\n",
        "        row = (i + 1) // cols + 1\n",
        "        col = (i + 1) % cols + 1\n",
        "        if col == 0:\n",
        "            col = cols\n",
        "            row -= 1\n",
        "\n",
        "        # Colors for this object\n",
        "        colors_gt = ['darkgreen', 'darkblue', 'darkred', 'darkorange', 'purple', 'brown']\n",
        "        colors_pred = ['lightgreen', 'lightblue', 'lightcoral', 'orange', 'violet', 'tan']\n",
        "\n",
        "        color_gt = colors_gt[i % len(colors_gt)]\n",
        "        color_pred = colors_pred[i % len(colors_pred)]\n",
        "\n",
        "        # Subsample points for performance\n",
        "        gt_points = pred['gt_points']\n",
        "        pred_points = pred['pred_points']\n",
        "\n",
        "        if len(gt_points) > 800:\n",
        "            indices = np.random.choice(len(gt_points), 800, replace=False)\n",
        "            gt_viz = gt_points[indices]\n",
        "            pred_viz = pred_points[indices]\n",
        "        else:\n",
        "            gt_viz = gt_points\n",
        "            pred_viz = pred_points\n",
        "\n",
        "        # Add GT and predicted points\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=gt_viz[:, 0], y=gt_viz[:, 1], z=gt_viz[:, 2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3, color=color_gt, opacity=0.8),\n",
        "                name=f'GT_{pred[\"object_name\"]}',\n",
        "                showlegend=True\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=pred_viz[:, 0], y=pred_viz[:, 1], z=pred_viz[:, 2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3, color=color_pred, opacity=0.8),\n",
        "                name=f'Pred_{pred[\"object_name\"]}',\n",
        "                showlegend=True\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        # Add coordinate axes\n",
        "        axis_length = 0.03\n",
        "        fig.add_trace(go.Scatter3d(x=[0, axis_length], y=[0, 0], z=[0, 0], mode='lines',\n",
        "                                   line=dict(color='red', width=4), showlegend=False), row=row, col=col)\n",
        "        fig.add_trace(go.Scatter3d(x=[0, 0], y=[0, axis_length], z=[0, 0], mode='lines',\n",
        "                                   line=dict(color='green', width=4), showlegend=False), row=row, col=col)\n",
        "        fig.add_trace(go.Scatter3d(x=[0, 0], y=[0, 0], z=[0, axis_length], mode='lines',\n",
        "                                   line=dict(color='blue', width=4), showlegend=False), row=row, col=col)\n",
        "\n",
        "    # Update layout\n",
        "    mean_add = total_add / valid_objects\n",
        "    mean_rot = total_rot / valid_objects\n",
        "\n",
        "    # Update scene properties for 3D subplots\n",
        "    for i in range(len(predictions)):\n",
        "        scene_name = f'scene{i+1}' if i > 0 else 'scene'\n",
        "        fig.update_layout(**{\n",
        "            scene_name: dict(\n",
        "                xaxis_title='X (m)',\n",
        "                yaxis_title='Y (m)',\n",
        "                zaxis_title='Z (m)',\n",
        "                aspectmode='cube',  # Back to cube for realistic proportions\n",
        "                camera=dict(\n",
        "                    eye=dict(x=1.5, y=1.5, z=1.5),  # Moderate camera distance\n",
        "                    center=dict(x=0, y=0, z=0),\n",
        "                    up=dict(x=0, y=0, z=1)\n",
        "                ),\n",
        "                # Keep realistic axis ranges\n",
        "                xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
        "                yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
        "                zaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
        "                bgcolor='white'\n",
        "            )\n",
        "        })\n",
        "\n",
        "    # Update main layout with reasonable dimensions\n",
        "    fig.update_layout(\n",
        "        title=dict(\n",
        "            text=f\"Enhanced Pose Estimation - Sample {sample_idx}<br>\" +\n",
        "                 f\"Objects: {valid_objects} | Mean ADD: {mean_add:.4f}m | Mean Rotation: {mean_rot:.1f}°\",\n",
        "            x=0.5,\n",
        "            font=dict(size=14)\n",
        "        ),\n",
        "        height=700 if len(predictions) <= 2 else 1000,  # Reasonable height\n",
        "        width=1600,  # Keep original width\n",
        "        showlegend=True,\n",
        "        margin=dict(l=20, r=20, t=80, b=20)  # Standard margins\n",
        "    )\n",
        "\n",
        "    # Hide axes for image subplot\n",
        "    fig.update_xaxes(showticklabels=False, showgrid=False, zeroline=False, row=1, col=1)\n",
        "    fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False, row=1, col=1)\n",
        "\n",
        "    # Show visualization\n",
        "    fig.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n✅ ENHANCED VISUALIZATION COMPLETE:\")\n",
        "    print(f\"   📊 Objects visualized: {valid_objects}\")\n",
        "    print(f\"   📈 Mean ADD: {mean_add:.6f}m\")\n",
        "    print(f\"   🔄 Mean Rotation Error: {mean_rot:.1f}°\")\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        rot_info = pred['rot_errors']\n",
        "        print(f\"      {i+1}. {pred['object_name']}:\")\n",
        "        print(f\"         ADD: {pred['add_error']:.4f}m\")\n",
        "        print(f\"         Rotation: {rot_info['overall']:.1f}° (X:{rot_info['x_axis']:.1f}° Y:{rot_info['y_axis']:.1f}° Z:{rot_info['z_axis']:.1f}°)\")\n",
        "\n",
        "    return {\n",
        "        'mean_add': mean_add,\n",
        "        'mean_rotation': mean_rot,\n",
        "        'num_objects': valid_objects,\n",
        "        'individual_results': predictions,\n",
        "        'image_path': rgb_path\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for obj_idx, object_info in enumerate(objects_in_image):\n",
        "            sample = create_sample_for_specific_object(dataset, object_info, rgb_image)\n",
        "            if sample is None:\n",
        "                continue\n",
        "\n",
        "            class_id = object_info['class_id']\n",
        "            object_names = dataset.data_config.get('names', [])\n",
        "            object_name = object_names[class_id] if class_id < len(object_names) else f\"Class_{class_id}\"\n",
        "\n",
        "            # Get prediction\n",
        "            rgb_batch = sample['rgb'].unsqueeze(0).to(config.DEVICE)\n",
        "            points_batch = sample['points'].unsqueeze(0).to(config.DEVICE)\n",
        "\n",
        "            try:\n",
        "                pred_poses, pred_confs = pose_model(rgb_batch, points_batch)\n",
        "                pred_pose = pred_poses[0].cpu().numpy()\n",
        "                gt_pose = sample['gt_pose'].cpu().numpy()\n",
        "\n",
        "                # Get model vertices\n",
        "                if class_id not in dataset.object_models:\n",
        "                    continue\n",
        "\n",
        "                vertices = dataset.object_models[class_id]['vertices']\n",
        "\n",
        "                # Compute ADD and rotation differences\n",
        "                if class_id in config.SYMMETRIC_LIST:\n",
        "                    add_error, pred_points, gt_points = compute_add_s_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "                else:\n",
        "                    add_error, pred_points, gt_points = compute_add_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "\n",
        "                rot_errors = compute_rotation_difference_degrees(pred_pose, gt_pose)\n",
        "\n",
        "                if pred_points is None:\n",
        "                    continue\n",
        "\n",
        "                predictions.append({\n",
        "                    'object_name': object_name,\n",
        "                    'class_id': class_id,\n",
        "                    'add_error': add_error,\n",
        "                    'rot_errors': rot_errors,\n",
        "                    'pred_points': pred_points,\n",
        "                    'gt_points': gt_points,\n",
        "                    'pred_pose': pred_pose,\n",
        "                    'gt_pose': gt_pose\n",
        "                })\n",
        "\n",
        "                total_add += add_error\n",
        "                total_rot += rot_errors['overall']\n",
        "                valid_objects += 1\n",
        "\n",
        "                print(f\"   ✅ {object_name}: ADD={add_error:.4f}m, Rot={rot_errors['overall']:.1f}°\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Prediction failed for {object_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if valid_objects == 0:\n",
        "        print(\"❌ No valid predictions to visualize\")\n",
        "        return None\n",
        "\n",
        "    # Create visualization\n",
        "    print(f\"\\n🎨 Creating enhanced visualization...\")\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n_objects = len(predictions)\n",
        "    if n_objects == 1:\n",
        "        rows, cols = 1, 2\n",
        "    elif n_objects == 2:\n",
        "        rows, cols = 2, 2\n",
        "    elif n_objects <= 4:\n",
        "        rows, cols = 2, 3\n",
        "    else:\n",
        "        rows = int(np.ceil((n_objects + 1) / 3))\n",
        "        cols = 3\n",
        "\n",
        "    # Create subplot specifications\n",
        "    subplot_specs = []\n",
        "    subplot_titles = []\n",
        "\n",
        "    # First subplot for original image\n",
        "    subplot_specs.append([{\"type\": \"xy\"}])\n",
        "    subplot_titles.append(\"Original Image with Detections\")\n",
        "\n",
        "    # Add 3D subplots for each object\n",
        "    for pred in predictions:\n",
        "        subplot_specs.append([{\"type\": \"scene\"}])\n",
        "        subplot_titles.append(f\"{pred['object_name']}\")\n",
        "\n",
        "    # Adjust specs for layout\n",
        "    if len(subplot_specs) <= 3:\n",
        "        spec_list = [[spec[0] for spec in subplot_specs]]\n",
        "    else:\n",
        "        spec_list = []\n",
        "        for i in range(0, len(subplot_specs), cols):\n",
        "            row_specs = []\n",
        "            for j in range(cols):\n",
        "                if i + j < len(subplot_specs):\n",
        "                    row_specs.append(subplot_specs[i + j][0])\n",
        "                else:\n",
        "                    row_specs.append({\"type\": \"xy\"})\n",
        "            spec_list.append(row_specs)\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(\n",
        "        rows=len(spec_list),\n",
        "        cols=cols,\n",
        "        subplot_titles=subplot_titles[:len(predictions) + 1],\n",
        "        specs=spec_list,\n",
        "        horizontal_spacing=0.05,\n",
        "        vertical_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # Add original image with bounding boxes\n",
        "    image_with_boxes = draw_bboxes_on_image(rgb_image, objects_in_image, predictions)\n",
        "    fig.add_trace(go.Image(z=image_with_boxes), row=1, col=1)\n",
        "\n",
        "    # Add 3D plots for each object\n",
        "    for i, pred in enumerate(predictions):\n",
        "        row = (i + 1) // cols + 1\n",
        "        col = (i + 1) % cols + 1\n",
        "        if col == 0:\n",
        "            col = cols\n",
        "            row -= 1\n",
        "\n",
        "        # Colors for this object\n",
        "        colors_gt = ['darkgreen', 'darkblue', 'darkred', 'darkorange', 'purple', 'brown']\n",
        "        colors_pred = ['lightgreen', 'lightblue', 'lightcoral', 'orange', 'violet', 'tan']\n",
        "\n",
        "        color_gt = colors_gt[i % len(colors_gt)]\n",
        "        color_pred = colors_pred[i % len(colors_pred)]\n",
        "\n",
        "        # Subsample points for performance\n",
        "        gt_points = pred['gt_points']\n",
        "        pred_points = pred['pred_points']\n",
        "\n",
        "        if len(gt_points) > 800:\n",
        "            indices = np.random.choice(len(gt_points), 800, replace=False)\n",
        "            gt_viz = gt_points[indices]\n",
        "            pred_viz = pred_points[indices]\n",
        "        else:\n",
        "            gt_viz = gt_points\n",
        "            pred_viz = pred_points\n",
        "\n",
        "        # Add GT and predicted points\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=gt_viz[:, 0], y=gt_viz[:, 1], z=gt_viz[:, 2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3, color=color_gt, opacity=0.8),\n",
        "                name=f'GT_{pred[\"object_name\"]}',\n",
        "                showlegend=True\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "                x=pred_viz[:, 0], y=pred_viz[:, 1], z=pred_viz[:, 2],\n",
        "                mode='markers',\n",
        "                marker=dict(size=3, color=color_pred, opacity=0.8),\n",
        "                name=f'Pred_{pred[\"object_name\"]}',\n",
        "                showlegend=True\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        # Add coordinate axes\n",
        "        axis_length = 0.03\n",
        "        fig.add_trace(go.Scatter3d(x=[0, axis_length], y=[0, 0], z=[0, 0], mode='lines',\n",
        "                                   line=dict(color='red', width=4), showlegend=False), row=row, col=col)\n",
        "        fig.add_trace(go.Scatter3d(x=[0, 0], y=[0, axis_length], z=[0, 0], mode='lines',\n",
        "                                   line=dict(color='green', width=4), showlegend=False), row=row, col=col)\n",
        "        fig.add_trace(go.Scatter3d(x=[0, 0], y=[0, 0], z=[0, axis_length], mode='lines',\n",
        "                                   line=dict(color='blue', width=4), showlegend=False), row=row, col=col)\n",
        "\n",
        "    # Update layout\n",
        "    mean_add = total_add / valid_objects\n",
        "    mean_rot = total_rot / valid_objects\n",
        "\n",
        "    # Update scene properties for 3D subplots\n",
        "    for i in range(len(predictions)):\n",
        "        scene_name = f'scene{i+1}' if i > 0 else 'scene'\n",
        "        fig.update_layout(**{\n",
        "            scene_name: dict(\n",
        "                xaxis_title='X (m)',\n",
        "                yaxis_title='Y (m)',\n",
        "                zaxis_title='Z (m)',\n",
        "                aspectmode='cube',\n",
        "                camera=dict(\n",
        "                    eye=dict(x=1.2, y=1.2, z=1.2),\n",
        "                    center=dict(x=0, y=0, z=0),\n",
        "                    up=dict(x=0, y=0, z=1)\n",
        "                )\n",
        "            )\n",
        "        })\n",
        "\n",
        "    # Update main layout\n",
        "    fig.update_layout(\n",
        "        title=f\"Enhanced Pose Estimation - Sample {sample_idx}<br>\" +\n",
        "              f\"Objects: {valid_objects} | Mean ADD: {mean_add:.4f}m | Mean Rotation: {mean_rot:.1f}°\",\n",
        "        height=600 if len(predictions) <= 2 else 900,\n",
        "        width=1600,\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Hide axes for image subplot\n",
        "    fig.update_xaxes(showticklabels=False, showgrid=False, zeroline=False, row=1, col=1)\n",
        "    fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False, row=1, col=1)\n",
        "\n",
        "    # Show visualization\n",
        "    fig.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n✅ ENHANCED VISUALIZATION COMPLETE:\")\n",
        "    print(f\"   📊 Objects visualized: {valid_objects}\")\n",
        "    print(f\"   📈 Mean ADD: {mean_add:.6f}m\")\n",
        "    print(f\"   🔄 Mean Rotation Error: {mean_rot:.1f}°\")\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        rot_info = pred['rot_errors']\n",
        "        print(f\"      {i+1}. {pred['object_name']}:\")\n",
        "        print(f\"         ADD: {pred['add_error']:.4f}m\")\n",
        "        print(f\"         Rotation: {rot_info['overall']:.1f}° (X:{rot_info['x_axis']:.1f}° Y:{rot_info['y_axis']:.1f}° Z:{rot_info['z_axis']:.1f}°)\")\n",
        "\n",
        "    return {\n",
        "        'mean_add': mean_add,\n",
        "        'mean_rotation': mean_rot,\n",
        "        'num_objects': valid_objects,\n",
        "        'individual_results': predictions,\n",
        "        'image_path': rgb_path\n",
        "    }\n",
        "\n",
        "def create_individual_3d_plots(pose_model, dataset, sample_idx=0):\n",
        "    \"\"\"Create individual 3D plots for each object with realistic proportions\"\"\"\n",
        "    print(f\"🎯 Creating individual 3D plots for sample {sample_idx}\")\n",
        "\n",
        "    # Validate inputs\n",
        "    if pose_model is None:\n",
        "        print(\"❌ Pose model is None\")\n",
        "        return None\n",
        "\n",
        "    if not hasattr(dataset, 'rgb_paths') or sample_idx >= len(dataset.rgb_paths):\n",
        "        print(\"❌ Invalid dataset or sample index\")\n",
        "        return None\n",
        "\n",
        "    # Get objects and process them\n",
        "    objects_in_image = get_all_objects_in_image(dataset, sample_idx)\n",
        "    if len(objects_in_image) == 0:\n",
        "        print(\"❌ No objects found\")\n",
        "        return None\n",
        "\n",
        "    # Load RGB image\n",
        "    rgb_path = dataset.rgb_paths[sample_idx]\n",
        "    rgb_image = cv2.imread(rgb_path)\n",
        "    if rgb_image is not None:\n",
        "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        print(f\"❌ Could not load image: {rgb_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📷 Image: {os.path.basename(rgb_path)}\")\n",
        "    print(f\"📦 Objects found: {len(objects_in_image)}\")\n",
        "\n",
        "    # Process predictions\n",
        "    pose_model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for obj_idx, object_info in enumerate(objects_in_image):\n",
        "            sample = create_sample_for_specific_object(dataset, object_info, rgb_image)\n",
        "            if sample is None:\n",
        "                continue\n",
        "\n",
        "            class_id = object_info['class_id']\n",
        "            object_names = dataset.data_config.get('names', [])\n",
        "            object_name = object_names[class_id] if class_id < len(object_names) else f\"Class_{class_id}\"\n",
        "\n",
        "            # Get prediction\n",
        "            rgb_batch = sample['rgb'].unsqueeze(0).to(config.DEVICE)\n",
        "            points_batch = sample['points'].unsqueeze(0).to(config.DEVICE)\n",
        "\n",
        "            try:\n",
        "                pred_poses, pred_confs = pose_model(rgb_batch, points_batch)\n",
        "                pred_pose = pred_poses[0].cpu().numpy()\n",
        "                gt_pose = sample['gt_pose'].cpu().numpy()\n",
        "\n",
        "                if class_id not in dataset.object_models:\n",
        "                    continue\n",
        "\n",
        "                vertices = dataset.object_models[class_id]['vertices_raw']\n",
        "                # Compute ADD and rotation differences\n",
        "                if class_id in config.SYMMETRIC_LIST:\n",
        "                    add_error, pred_points, gt_points = compute_add_s_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "                else:\n",
        "                    add_error, pred_points, gt_points = compute_add_visualization_fixed(\n",
        "                        pred_pose, gt_pose, vertices\n",
        "                    )\n",
        "                rot_errors = compute_rotation_difference_degrees(pred_pose, gt_pose)\n",
        "\n",
        "                if pred_points is not None:\n",
        "                    predictions.append({\n",
        "                        'object_name': object_name,\n",
        "                        'class_id': class_id,\n",
        "                        'add_error': add_error,\n",
        "                        'rot_errors': rot_errors,\n",
        "                        'pred_points': pred_points,\n",
        "                        'gt_points': gt_points\n",
        "                    })\n",
        "\n",
        "                    print(f\"   ✅ {object_name}: ADD={add_error:.4f}m, Rot={rot_errors['overall']:.1f}°\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Failed for {object_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if len(predictions) == 0:\n",
        "        print(\"❌ No valid predictions\")\n",
        "        return None\n",
        "\n",
        "    # Create individual 3D plots for each object\n",
        "    print(f\"🎨 Creating {len(predictions)} individual 3D plots...\")\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        # Create individual figure for each object\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Get points\n",
        "        gt_points = pred['gt_points']\n",
        "        pred_points = pred['pred_points']\n",
        "\n",
        "        # Subsample for performance\n",
        "        if len(gt_points) > 800:\n",
        "            indices = np.random.choice(len(gt_points), 800, replace=False)\n",
        "            gt_viz = gt_points[indices]\n",
        "            pred_viz = pred_points[indices]\n",
        "        else:\n",
        "            gt_viz = gt_points\n",
        "            pred_viz = pred_points\n",
        "\n",
        "        # Add ground truth points\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=gt_viz[:, 0], y=gt_viz[:, 1], z=gt_viz[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=4, color='darkgreen', opacity=0.8),\n",
        "            name='Ground Truth',\n",
        "            showlegend=True\n",
        "        ))\n",
        "\n",
        "        # Add predicted points\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=pred_viz[:, 0], y=pred_viz[:, 1], z=pred_viz[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=4, color='red', opacity=0.7),\n",
        "            name='Prediction',\n",
        "            showlegend=True\n",
        "        ))\n",
        "\n",
        "        # Add coordinate axes\n",
        "        axis_length = 0.05\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[0, axis_length], y=[0, 0], z=[0, 0],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=6),\n",
        "            name='X-axis',\n",
        "            showlegend=True\n",
        "        ))\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[0, 0], y=[0, axis_length], z=[0, 0],\n",
        "            mode='lines',\n",
        "            line=dict(color='green', width=6),\n",
        "            name='Y-axis',\n",
        "            showlegend=True\n",
        "        ))\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=[0, 0], y=[0, 0], z=[0, axis_length],\n",
        "            mode='lines',\n",
        "            line=dict(color='blue', width=6),\n",
        "            name='Z-axis',\n",
        "            showlegend=True\n",
        "        ))\n",
        "\n",
        "        # Update layout with realistic proportions\n",
        "        fig.update_layout(\n",
        "            title=dict(\n",
        "                text=f\"{pred['object_name']} - Sample {sample_idx}<br>\" +\n",
        "                     f\"ADD: {pred['add_error']:.4f}m | Rotation: {pred['rot_errors']['overall']:.1f}°<br>\" +\n",
        "                     f\"Per-axis Rot: X:{pred['rot_errors']['x_axis']:.1f}° Y:{pred['rot_errors']['y_axis']:.1f}° Z:{pred['rot_errors']['z_axis']:.1f}°\",\n",
        "                x=0.5,\n",
        "                font=dict(size=14)\n",
        "            ),\n",
        "            scene=dict(\n",
        "                xaxis=dict(\n",
        "                    title='X (meters)',\n",
        "                    showgrid=True,\n",
        "                    gridwidth=1,\n",
        "                    gridcolor='lightgray'\n",
        "                ),\n",
        "                yaxis=dict(\n",
        "                    title='Y (meters)',\n",
        "                    showgrid=True,\n",
        "                    gridwidth=1,\n",
        "                    gridcolor='lightgray'\n",
        "                ),\n",
        "                zaxis=dict(\n",
        "                    title='Z (meters)',\n",
        "                    showgrid=True,\n",
        "                    gridwidth=1,\n",
        "                    gridcolor='lightgray'\n",
        "                ),\n",
        "                aspectmode='cube',  # Keep realistic proportions\n",
        "                camera=dict(\n",
        "                    eye=dict(x=1.5, y=1.5, z=1.5),  # Standard camera position\n",
        "                    center=dict(x=0, y=0, z=0),\n",
        "                    up=dict(x=0, y=0, z=1)\n",
        "                ),\n",
        "                bgcolor='white'\n",
        "            ),\n",
        "            width=900,\n",
        "            height=700,\n",
        "            showlegend=True,\n",
        "            margin=dict(l=0, r=0, t=100, b=0)\n",
        "        )\n",
        "\n",
        "        # Show the plot\n",
        "        fig.show()\n",
        "\n",
        "        print(f\"📊 Plot {i+1}/{len(predictions)}: {pred['object_name']}\")\n",
        "\n",
        "    # Also create the original image with bounding boxes\n",
        "    image_with_boxes = draw_bboxes_on_image(rgb_image, objects_in_image, predictions)\n",
        "\n",
        "    # Show original image\n",
        "    fig_img = go.Figure()\n",
        "    fig_img.add_trace(go.Image(z=image_with_boxes))\n",
        "    fig_img.update_layout(\n",
        "        title=f\"Original Image with Detections - Sample {sample_idx}\",\n",
        "        width=800,\n",
        "        height=600,\n",
        "        xaxis=dict(showticklabels=False, showgrid=False, zeroline=False),\n",
        "        yaxis=dict(showticklabels=False, showgrid=False, zeroline=False)\n",
        "    )\n",
        "    fig_img.show()\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMSJMJAu9n_t"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 12: MAIN EXECUTION PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "def main_pipeline(mode='complete'):\n",
        "    \"\"\"\n",
        "    Main execution pipeline for DenseFusion inspired network\n",
        "\n",
        "    Args:\n",
        "        mode: 'complete' (train+eval), 'train_only', 'eval_only', or 'visualize_only'\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"DENSEFUSION 6D POSE ESTIMATION - MAIN PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"Configuration Summary:\")\n",
        "    config.print_config()\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        # Step 1: Verify setup\n",
        "        print(\"\\nStep 1: Verifying Setup\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if not config.verify_paths():\n",
        "            print(\"⚠ Please fix path configuration before proceeding\")\n",
        "            return None\n",
        "\n",
        "        print(\"✓ All paths verified\")\n",
        "\n",
        "        # Step 2: Training (if requested)\n",
        "        if mode in ['complete', 'train_only']:\n",
        "            print(\"\\nStep 2: Training DenseFusion Model\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            try:\n",
        "                training_results = train_densefusion()\n",
        "                results['training'] = training_results\n",
        "                print(\"✓ Training completed successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Training failed: {e}\")\n",
        "                if mode == 'train_only':\n",
        "                    return results\n",
        "        else:\n",
        "            print(\"\\nStep 2: Skipping Training\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        # Step 3: Evaluation (if requested)\n",
        "        if mode in ['complete', 'eval_only']:\n",
        "            print(\"\\nStep 3: Model Evaluation\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            try:\n",
        "                eval_results = run_complete_evaluation()\n",
        "                if eval_results is not None:\n",
        "                    results['evaluation'] = eval_results\n",
        "                    print(\"✓ Evaluation completed successfully\")\n",
        "\n",
        "                    # Print key metrics\n",
        "                    print(f\"\\nKey Results:\")\n",
        "                    print(f\"  Detection Rate: {eval_results.get('detection_rate', 0):.3f}\")\n",
        "                    print(f\"  Mean ADD Error: {eval_results.get('mean_add', float('inf')):.4f} m\")\n",
        "                    print(f\"  Success Rate (<5cm): {eval_results.get('success_rate_5cm', 0):.3f}\")\n",
        "                    print(f\"  Success Rate (<10cm): {eval_results.get('success_rate_10cm', 0):.3f}\")\n",
        "                else:\n",
        "                    print(\"✗ Evaluation failed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Evaluation error: {e}\")\n",
        "        else:\n",
        "            print(\"\\nStep 3: Skipping Evaluation\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        # Step 4: Visualization (if requested)\n",
        "        if mode in ['complete', 'visualize_only']:\n",
        "            print(\"\\nStep 4: Enhanced Visualization\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            try:\n",
        "                # Run visualization on a few sample indices\n",
        "                sample_indices = [0, 100, 200, 500, 1000]\n",
        "                viz_results = visualize_enhanced_all_objects(sample_idx=sample_indices[0]) #visualize only 1\n",
        "                results['visualization'] = viz_results\n",
        "                print(f\"✓ Visualization completed for {len(viz_results)} samples\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Visualization error: {e}\")\n",
        "        else:\n",
        "            print(\"\\nStep 4: Skipping Visualization\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        # Step 5: Final Summary\n",
        "        print(\"\\nStep 5: Final Summary\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if results:\n",
        "            print(\"Pipeline completed successfully!\")\n",
        "\n",
        "            if 'training' in results:\n",
        "                training = results['training']\n",
        "                print(f\"\\nTraining Results:\")\n",
        "                print(f\"  Epochs completed: {len(training.get('train_losses', []))}\")\n",
        "                print(f\"  Best validation loss: {training.get('best_val_loss', 'N/A'):.6f}\")\n",
        "                print(f\"  Training time: {training.get('total_time', 0):.1f} seconds\")\n",
        "\n",
        "            if 'evaluation' in results:\n",
        "                evaluation = results['evaluation']\n",
        "                print(f\"\\nEvaluation Results:\")\n",
        "                print(f\"  Detection rate: {evaluation.get('detection_rate', 0):.3f}\")\n",
        "                print(f\"  Mean ADD error: {evaluation.get('mean_add', float('inf')):.4f} m\")\n",
        "                print(f\"  Success rate (<5cm): {evaluation.get('success_rate_5cm', 0):.3f}\")\n",
        "                print(f\"  Success rate (<10cm): {evaluation.get('success_rate_10cm', 0):.3f}\")\n",
        "\n",
        "            if 'visualization' in results:\n",
        "                viz = results['visualization']\n",
        "                print(f\"\\nVisualization Results:\")\n",
        "                print(f\"  Samples visualized: {len(viz)}\")\n",
        "                if viz:\n",
        "                    mean_add = np.mean([r['mean_add'] for r in viz if 'mean_add' in r])\n",
        "                    mean_rot = np.mean([r['mean_rotation'] for r in viz if 'mean_rotation' in r])\n",
        "                    print(f\"  Average ADD error: {mean_add:.4f} m\")\n",
        "                    print(f\"  Average rotation error: {mean_rot:.1f}°\")\n",
        "        else:\n",
        "            print(\"Pipeline completed with no results\")\n",
        "\n",
        "        print(f\"\\nFiles saved to: {config.MODELS_SAVE_DIR}\")\n",
        "        print(\"=\"*80)\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Pipeline failed with error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return results\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test of all components\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"QUICK TEST - DENSEFUSION COMPONENTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    tests = [\n",
        "        (\"Configuration\", lambda: config.verify_paths()),\n",
        "        (\"Dataset Loading\", lambda: test_dataset() is not None),\n",
        "        (\"Model Architecture\", lambda: test_model_architecture() is not None),\n",
        "        (\"Loss Function\", lambda: test_loss_function() is not None),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for test_name, test_func in tests:\n",
        "        print(f\"\\nTesting {test_name}...\")\n",
        "        try:\n",
        "            result = test_func()\n",
        "            results[test_name] = result\n",
        "            status = \"✓ PASS\" if result else \"✗ FAIL\"\n",
        "            print(f\"{test_name}: {status}\")\n",
        "        except Exception as e:\n",
        "            results[test_name] = False\n",
        "            print(f\"{test_name}: ✗ FAIL ({e})\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"QUICK TEST SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    passed = sum(1 for r in results.values() if r)\n",
        "    total = len(results)\n",
        "\n",
        "    for test_name, result in results.items():\n",
        "        status = \"✓\" if result else \"✗\"\n",
        "        print(f\"  {status} {test_name}\")\n",
        "\n",
        "    print(f\"\\nOverall: {passed}/{total} tests passed\")\n",
        "\n",
        "    if passed == total:\n",
        "        print(\"🎉 All tests passed! Ready to run main pipeline.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"⚠ Some tests failed. Fix issues before running main pipeline.\")\n",
        "        return False\n",
        "\n",
        "def load_and_use_pretrained_model(model_path=None, sample_idx=0):\n",
        "    \"\"\"Load a pretrained model and run inference on a sample\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"LOADING AND USING PRETRAINED MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load model\n",
        "    model = load_trained_model(model_path, use_transformer=config.USE_TRANSFORMER_FUSIO)\n",
        "    if model is None:\n",
        "        print(\"❌ Failed to load model\")\n",
        "        return None\n",
        "\n",
        "    # Load dataset for testing\n",
        "    dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "    test_dataset = DenseFusionDataset(\n",
        "        dataset_config, split='test', use_segmentation=config.USE_SEGMENTATION\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Model loaded successfully\")\n",
        "    print(f\"✓ Test dataset loaded: {len(test_dataset)} samples\")\n",
        "\n",
        "    # Run inference on a sample\n",
        "    try:\n",
        "        sample = test_dataset[sample_idx]\n",
        "        rgb = sample['rgb'].unsqueeze(0).to(config.DEVICE)\n",
        "        points = sample['points'].unsqueeze(0).to(config.DEVICE)\n",
        "        gt_pose = sample['gt_pose']\n",
        "        class_id = sample['class_id'].item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_pose, pred_conf = model(rgb, points)\n",
        "\n",
        "        pred_pose_np = pred_pose.cpu().numpy().flatten()\n",
        "        confidence = torch.sigmoid(pred_conf).cpu().numpy().item()\n",
        "\n",
        "        print(f\"\\n📊 Inference Results for Sample {sample_idx}:\")\n",
        "        print(f\"  Class ID: {class_id}\")\n",
        "        print(f\"  Predicted pose: {pred_pose_np}\")\n",
        "        print(f\"  Ground truth pose: {gt_pose.numpy()}\")\n",
        "        print(f\"  Confidence: {confidence:.4f}\")\n",
        "\n",
        "        # Compute ADD if model available\n",
        "        if class_id in test_dataset.object_models:\n",
        "            vertices = test_dataset.object_models[class_id]['vertices_raw']\n",
        "            # Compute ADD and rotation differences\n",
        "            if class_id in config.SYMMETRIC_LIST:\n",
        "                add_error = compute_add_s_metric(pred_pose_np, gt_pose.numpy(), vertices)\n",
        "            else:\n",
        "                add_error = compute_add_metric(pred_pose_np, gt_pose.numpy(), vertices)\n",
        "            print(f\"  ADD error: {add_error:.6f} m\")\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'sample_idx': sample_idx,\n",
        "            'pred_pose': pred_pose_np,\n",
        "            'gt_pose': gt_pose.numpy(),\n",
        "            'confidence': confidence,\n",
        "            'class_id': class_id\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Inference failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def simple_visualization(sample_idx=0):\n",
        "    \"\"\"Simple visualization function for quick testing\"\"\"\n",
        "    try:\n",
        "        # Load necessary components\n",
        "        dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "        test_dataset = DenseFusionDataset(dataset_config, split='test', use_segmentation=config.USE_SEGMENTATION)\n",
        "        model = load_trained_model(use_transformer=config.USE_TRANSFORMER_FUSION)\n",
        "\n",
        "        if model is None:\n",
        "            print(\"⚠ No trained model found for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Run enhanced visualization\n",
        "        result = visualize_enhanced_all_objects(model, test_dataset, sample_idx)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Visualization failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_model(use_transformer=config.USE_TRANSFORMER_FUSION):\n",
        "    \"\"\"Simple function to just train the model\"\"\"\n",
        "    if use_transformer is not None:\n",
        "        use_transformer=config.USE_TRANSFORMER_FUSION\n",
        "    return main_pipeline(mode='train_only')\n",
        "\n",
        "def evaluate_model(use_transformer=config.USE_TRANSFORMER_FUSION):\n",
        "    \"\"\"Simple function to just evaluate the model\"\"\"\n",
        "    if use_transformer is not None:\n",
        "        use_transformer=config.USE_TRANSFORMER_FUSION\n",
        "    return main_pipeline(mode='eval_only')\n",
        "\n",
        "def visualize_model(use_transformer=config.USE_TRANSFORMER_FUSION):\n",
        "    \"\"\"Simple function to just run visualization\"\"\"\n",
        "    if use_transformer is not None:\n",
        "        use_transformer=config.USE_TRANSFORMER_FUSION\n",
        "    return main_pipeline(mode='visualize_only')\n",
        "\n",
        "def run_full_pipeline(use_transformer=config.USE_TRANSFORMER_FUSION):\n",
        "    \"\"\"Run the complete pipeline\"\"\"\n",
        "    if use_transformer is not None:\n",
        "        use_transformer=config.USE_TRANSFORMER_FUSION\n",
        "    return main_pipeline(mode='complete')\n",
        "\n",
        "# Print usage information\n",
        "print(\"✓ Block 12 completed: Main execution pipeline ready\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DENSEFUSION IMPLEMENTATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  🧪 quick_test() - Test all components\")\n",
        "print(\"  🚀 run_full_pipeline() - Complete training and evaluation\")\n",
        "print(\"  🏋️ train_model() - Training only\")\n",
        "print(\"  📊 evaluate_model() - Evaluation only\")\n",
        "print(\"  🎨 visualize_model() - Visualization only\")\n",
        "print(\"  📱 simple_visualization(sample_idx) - Quick visualization\")\n",
        "print(\"  🔧 load_and_use_pretrained_model(model_path) - Load and test pretrained model\")\n",
        "print(\"\\nRecommended usage:\")\n",
        "print(\"  1. quick_test() - Verify everything works\")\n",
        "print(\"  2. run_full_pipeline() - Complete pipeline\")\n",
        "print(\"  3. simple_visualization(sample_idx) - Visualize results\")\n",
        "print(\"\\nTo load existing model:\")\n",
        "print(\"  result = load_and_use_pretrained_model('/path/to/model.pth')\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edzv8Brz9oCf"
      },
      "outputs": [],
      "source": [
        "# Run a comprehensive test for all the previous blocks\n",
        "quick_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxdvi2XxKkTM"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 13: MODEL TRAINING\n",
        "# ==============================================================================\n",
        "run_full_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlA0ZDatK5rk"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 14: MODEL EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Load dataset config first\n",
        "dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "\n",
        "#IF NEEDED ANOTHER NAME FOR THE MODEL\n",
        "#config.MODEL_NAME = \"densefusion_custom_v2\"  # Your custom name\n",
        "\n",
        "# Create and load model\n",
        "pose_model = DenseFusionNetwork(num_objects=13,use_transformer=config.USE_TRANSFORMER_FUSION)\n",
        "pose_model.load_state_dict(torch.load(os.path.join(config.MODELS_SAVE_DIR, config.MODELS_NAME+'_densefusion_best.pth'), map_location=config.DEVICE))\n",
        "pose_model = pose_model.to(config.DEVICE)\n",
        "pose_model.eval()\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = DenseFusionDataset(\n",
        "    dataset_config,\n",
        "    split='test',\n",
        "    num_points=config.NUM_POINTS,\n",
        "    patch_size=config.PATCH_SIZE,\n",
        "    use_segmentation=config.USE_SEGMENTATION\n",
        ")\n",
        "\n",
        "# Load YOLO model and diameters\n",
        "yolo_model = load_yolo_model(config.YOLO_MODEL_PATH)\n",
        "model_diameters = load_model_diameters(config.DIAMETER_INFO_PATH)\n",
        "\n",
        "# Set evaluation to ALL samples\n",
        "config.MAX_EVAL_SAMPLES = len(test_dataset)\n",
        "\n",
        "# Run evaluation on all test data\n",
        "metrics = evaluate_model_comprehensive(yolo_model, pose_model, test_dataset, model_diameters)\n",
        "\n",
        "# Print comprehensive results\n",
        "print(f\"Complete Evaluation Results on {len(test_dataset)} samples:\")\n",
        "print(f\"Detection rate: {metrics['detection_rate']:.3f}\")\n",
        "print(f\"Mean ADD error: {metrics['mean_add']:.4f} m\")\n",
        "print(f\"Success rate (<2cm): {metrics['success_rate_2cm']:.3f}\")\n",
        "print(f\"Success rate (<5cm): {metrics['success_rate_5cm']:.3f}\")\n",
        "print(f\"Success rate (<10cm): {metrics['success_rate_10cm']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkm9j3MjLQWW"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# BLOCK 15: SAMPLE VISUALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Load dataset config first #RESULTS WITH TRANSFORMERS\n",
        "config.USE_TRANSFORMER_FUSION\n",
        "config.USE_SEGMENTATION\n",
        "dataset_config = load_dataset_config(config.LINEMOD_ROOT)\n",
        "\n",
        "# Create and load model - FIXED\n",
        "pose_model = DenseFusionNetwork(num_objects=13, use_transformer=config.USE_TRANSFORMER_FUSION)\n",
        "pose_model.load_state_dict(torch.load(os.path.join(config.MODELS_SAVE_DIR, 'EC_500_512_MLP_densefusion_best.pth'), map_location=config.DEVICE))\n",
        "pose_model = pose_model.to(config.DEVICE)\n",
        "pose_model.eval()\n",
        "\n",
        "# Create test dataset - FIXED\n",
        "test_dataset = DenseFusionDataset(\n",
        "    dataset_config,\n",
        "    split='test',\n",
        "    num_points=config.NUM_POINTS,      # lowercase config\n",
        "    patch_size=config.PATCH_SIZE,      # lowercase config\n",
        "    use_segmentation=config.USE_SEGMENTATION  # lowercase config\n",
        ")\n",
        "\n",
        "# Visualize\n",
        "#result = create_individual_3d_plots(pose_model, test_dataset, sample_idx=1)\n",
        "result=create_individual_3d_plots(pose_model, test_dataset, sample_idx=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}